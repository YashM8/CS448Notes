# Overparameterization

## Overparameterization and SGD

Overparameterization means that the model can fit the entire training data. Modern machine learning models like neural networks tend to do very well in overparameterized settings. Typically, over complicated models, you tend to overfit.


In the overparameterized regime we have $\nabla f_i (w_*) = 0$ for all $i$. The variance is 0 at minimizers and SGD converges with a small enough step size.

One way to characterize over-parameterization: strong growth condition (SGC) -

$$
E[\|\nabla f_i(w)\|^2] \leq \rho \|\nabla f(w)\|^2, \\
\text{This implies the interpolation property that } \nabla f(w) = 0 \Rightarrow \nabla f_i(w) = 0 \text{ for all } i.
$$
**Bound on Error under the SGC -**

Under the SGC, the SGD error is bounded by the full gradient size

$$
E[\|e_k\|^2] = E[\|\nabla f_i(w_k) - \nabla f(w_k)\|^2] \\
= \|\nabla f(w_k)\|^2 - \|\nabla f_i(w_k)\|^2 \\
= \frac{1}{n} \sum_{i=1}^n \|\nabla f_i(w_k)\|^2 - \|\nabla f_i(w_k)\|^2 \\
\leq \frac{1}{n} \sum_{i=1}^n \rho \|\nabla f(w_k)\|^2 - \|\nabla f(w_k)\|^2 \quad (\text{using SGC}) \\
= (\rho - 1) \|\nabla f(w_k)\|^2 \\
\text{So under the SGC, we do not need an assumption like } E[\|e_k\|^2] \leq \sigma^2.
$$

Using SGD in descent lemma with $\alpha_k = \frac{1}{L\rho}$ with SGC we obtain -

$$
E[f(w_{k+1})] \leq f(w_k) - \frac{1}{2L\rho} \|\nabla f(w_k)\|^2, \\
$$

The function decrease of deterministic gradient descent up to a factor of $\rho$. Any step size $\alpha < \frac{2}{L}\rho$ guarantees descent.

## Faster SGD for Overparameterized Models
 
Over-parameterization leads to faster convergence rates for SGD. There now exist methods that go faster in over-parameterized setting. Under over-parameterization, you can also use SGD with a line-search.

The setting of SGD step size as we go works broadly as -

- “Update step size based on some simple statistics”.

- “Do gradient descent on the step size”.

- “Use a line-search/trust-region based on the mini-batch”.

Most of these methods have problems like -

- Introduces new hyper-parameters that are just as hard to tune as the step size.

- Do not converge theoretically.

- Converges theoretically, but works badly in practice.

- Needs to assume that $\sigma_k$ goes to 0 to work

## Stochastic Line Search

Armijo line-search on the mini-batch selects a step size satisfying - 

$$
f_i(w_k - \alpha_k \nabla f_i(w_k)) \leq f_i(w_k) - c \alpha_k \|\nabla f_i(w_k)\|^2, 
\text{for some constant } c > 0. \\
\text{}
$$

Without interpolation this does not work (satisfied by steps that are too large). With interpolation, can guarantee sufficient progress towards solution.

Line search works well if the model is overparameterized or close to over overparameterized but works horribly if it is far from overparameterized.





