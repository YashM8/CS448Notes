[{"path":"index.html","id":"cpsc-448-notes","chapter":"1 CPSC 448 Notes","heading":"1 CPSC 448 Notes","text":"Directed studies optimization ML advanced deep learning architectures.","code":""},{"path":"gradient-descent-analysis.html","id":"gradient-descent-analysis","chapter":"2 Gradient Descent Analysis","heading":"2 Gradient Descent Analysis","text":"","code":""},{"path":"gradient-descent-analysis.html","id":"gradient-descent-background","chapter":"2 Gradient Descent Analysis","heading":"2.1 Gradient descent background","text":"Gradient descent iterative optimization algorithm first proposed 1847 Cauchy. algorithm can summarized follows:\\[\nw^t = w^{t-1} - \\alpha_t \\nabla f(w^t) \\\\\n\\text{  t = 1, 2, 3 ...}\n\\]trying find set parameters \\(w\\) minimize function \\(f\\), also called objective function. \\(\\nabla f\\) gradient \\(f\\) respect \\(w\\) subscript \\(t\\) iteration number.main idea move opposite direction steepest ascent function \\(f\\). much move iteration controlled two things. first steepness gradient control chosen objective function. second parameter \\(\\alpha_t\\). also called th learning rate.time complexity iteration \\(O(d)\\) computing gradient. \\(d\\) number parameters. can stop made little progress, \\(||f(w^t) - f(w^{t-1})||\\) small.Intuitively gradient descent can thought standing top canyon blindfolded using leg find steepest slope downwards locally. , taking small step direction repeating process reach bottom. analogy canyon can thought modelled 3D objective function.","code":""},{"path":"gradient-descent-analysis.html","id":"showing-gradient-descent-reduces-the-objective-function.","chapter":"2 Gradient Descent Analysis","heading":"2.2 Showing gradient descent reduces the objective function.","text":"Let us assume objective function Lipschitz continuous. Intuitively means function change gradient arbitrarily fast. Formally means real valued number \\(L\\) satisfies:\\[\n\\nabla f(w) − \\nabla f(v) \\le L||w − v||\n\\]\ntwice continuously differentiable \\((C^2)\\) functions, using mean value theorem, can show:\n\\[\n||\\nabla^2 f (w)|| \\le L \\Longrightarrow \\nabla^2 f (w) \\le LI\n\\]\ncan bound quadratic functions form using:\\[\nd^T \\nabla^2f(w) d ≤ d^T (LI)d = Ld^Td = L||d^2||\n\\]\nUsing multivariate Taylor expansion:\\[\nf(v) = f(w) + \\nabla f(w)^T (v - w) + \\frac{1}{2} (v - w)^T \\nabla^2 f(u) (v - w),\\text{} u \\[v, w]\n\\\\\nf(v) \\le f(w) + \\nabla f(w)^T (v - w) + \\frac{L}{2} ||v-w||^2\n\\]\nalso known descent lemma.inequality give us upper bound \\(f\\) can minimized \\(\\alpha_t = \\frac{1}{L}\\). Using equations can show gradient descent reduces objective one iteration:\\[\nw^t = w^{t-1} - \\alpha_t \\nabla f(w^k) \\\\\nw^t = w^{t-1} - \\frac{1}{L} \\nabla f(w^t)\\\\\nf(w^t) \\le f(w^{t-1}) + \\nabla f(w^{t-1})^T (w^{t} - w^{t-1}) + \\frac{L}{2} ||w^{t}-w^{t-1}||^2\\\\\n\\text{Now can use, } w^{t}-w^{t-1} = - \\frac{1}{L} \\nabla f(w^{t-1})\\\\\nf(w^t) \\le f(w^{t-1}) - \\nabla f(w^{t-1})^T \\frac{1}{L} \\nabla f(w^{t-1}) + \\frac{L}{2} ||\\frac{1}{L} \\nabla f(w^{t-1})||^2\\\\\nf(w^t) \\le f(w^{t-1}) - \\frac{1}{L} ||\\nabla f(w^{t-1})||^2 + \\frac{1}{2L} ||\\nabla f(w^{t-1})||^2\\\\\nf(w^t) \\le f(w^{t-1}) - \\frac{1}{2L} ||\\nabla f(w^{t-1})||^2\\\\\n\\]\nshows every iteration guaranteed make progress learning rate \\(\\alpha_t = \\frac{1}{L}\\) gradient non-zero.","code":""},{"path":"gradient-descent-analysis.html","id":"what-learning-rate-to-use","chapter":"2 Gradient Descent Analysis","heading":"2.3 What learning rate to use?","text":"Using \\(\\alpha_t = \\frac{1}{L}\\) impractical since computing \\(L\\) expensive. step size get approach usually small. practical solution approximate \\(L\\). Starting initial guess \\(\\hat{L}\\). take step, check progress bound satisfied.\\[\nf(w^t -\\frac{1}{\\hat{L}} \\nabla f(w^{t})) \\le f(w^{t}) - \\frac{1}{2L} ||\\nabla f(w^{t})||^2\\\\\n\\text{} w^t -\\frac{1}{\\hat{L}} f(w^{t}) \\text{ potential } w^{t+1}\n\\]\n, double \\(\\hat{L}\\) condition satisfied.Armijo BacktrackingStart iteration Start iteration large \\(\\alpha\\) optimistic fact worst case need small step size.Start iteration Start iteration large \\(\\alpha\\) optimistic fact worst case need small step size.Half \\(\\alpha\\) Armijo condition satisfies. given :Half \\(\\alpha\\) Armijo condition satisfies. given :\\[\nf(w^t - \\alpha \\nabla f(w^{t})) \\le f(w^t) - \\alpha \\gamma ||\\nabla f(w^t)||^2 \\\\\n\\text{} \\gamma \\(0, \\frac{1}{2}]\n\\]allows us vary learning rate way new set parameters \\(w^{t+1}\\) sufficiently decreased objective function going current parameters \\(w^t\\).come later.","code":""},{"path":"gradient-descent-analysis.html","id":"convergence-rate","chapter":"2 Gradient Descent Analysis","heading":"2.4 Convergence rate","text":"Using progress bound, \\(f(w^t) \\le f(w^{t-1}) - \\frac{1}{2L} ||\\nabla f(w^{t-1})||^2 \\Longrightarrow ||\\nabla f(w^{t-1})||^2 \\le 2L[f(w^{t-1}) - f(w^t)]\\)Let’s consider smallest squared gradient norm \\(\\min_{\\mathbf{j \\{[0, t-1]}}} \\|\\nabla f(\\mathbf{w^j})\\|^2\\). change objective function smallest.Trivally, smaller average squared gradient norm:\\[\n\\min_{\\mathbf{j \\{[0, t-1]}}} \\|\\nabla f({w^j})\\|^2 \\le \\frac{1}{t} \\sum_{k=1}^t ||\\nabla f({w^{k-1}})||^2 \\le \\frac{2L}{t} \\sum_{k=1}^t [f({w^{k-1}}) - f(w^k)] \\\\\n\\frac{2L}{t} \\sum_{k=1}^t [f({w^{k-1}}) - f(w^k)] = \\frac{2L}{t} [f(w^0) - f(w^t)], \\text{ Since telescoping sum}\\\\\n\\text{Also, } f(w^t) \\ge f^* \\text{ } f^* \\text{ objective value optimal } w^* \\\\\n\\min_{\\mathbf{j \\{[0, t-1]}}} \\|\\nabla f({w^j})\\|^2 \\le \\frac{2L}{t} [f(w^0) - f^*] = O(1/t)\n\\]\nlast iteration satisfies inequality. can satisfied point optimization process. last iteration however lowest \\(f\\) value. also imply find global optima. minimizing objective local optima.usually stop iterative process norm small value \\(\\epsilon\\).\\[\n\\min_{\\mathbf{j \\{[0, t-1]}}} \\|\\nabla f({w^j})\\|^2 \\le \\frac{2L}{t} [f(w^0) - f^*] \\le \\epsilon \\\\\nt \\geq \\frac{2L[f(w_0) - f^*)]}{\\epsilon}\\\\\nt = O(1/\\epsilon)\n\\]\nsatisfy stopping condition. \\(t = O(1/\\epsilon)\\) called iteration complexity algorithm. least squares, cost computing gradient \\(O(nd)\\) \\(n\\) nuber data points \\(d\\) dimensionality data. total cost \\(O(nd \\times 1/\\epsilon)\\)Another way measure rate convergence limit ratio successive errors:\n\\[\n\\lim_{k \\\\infty} \\frac{f(w_{k+1}) - f(w^*)}{f(w_k) - f(w^*)} = \\rho.\n\\]\nDifferent values \\(\\rho\\) give us different rates convergence:\\(\\rho=1\\), called sublinear rate. means need \\(O(1/\\epsilon)\\) iterations.\\(\\rho \\(0, 1)\\) called linear rate. means need \\(O(log(1/\\epsilon))\\) iterations.\\(\\rho = 0\\), called superlinear rate. means need \\(O(log(log(1/\\epsilon))\\) iterations.\\(f(w_t) - f(w^*) = O(1/t)\\) gives sublinear convergence rate. longer run algorithm, less progress makes.Polyak-Lojasiewicz (PL) InequalityGradient descent least squares linear cost sublinear rate. many “nice” functions, gradient descent actually linear rate. example, functions satisfying PL Inequality:\\[\n\\frac{1}{2} ||\\nabla f(w)||^2 \\ge \\mu (f(w) - f^*)\n\\]\nget linear convergence rate PL ineqaulity:\\[\nf(w^{k+1}) \\leq f(w^k) - \\frac{1}{2L} \\|\\nabla f(w^k)\\|^2. \\\\\n\\text{PL inequality, :} \\\\\n-\\|\\nabla f(w^k)\\|^2 \\leq -2\\mu (f(w^k) - f^*). \\\\\nf(w^{k+1}) \\leq f(w^k) - \\frac{\\mu}{L} (f(w^k) - f^*). \\\\\nf(w^{k+1}) - f^* \\leq f(w^k) - f^* - \\frac{\\mu}{L} (f(w^k) - f^*). \\\\\nf(w^{k+1}) - f^* \\leq \\left(1 - \\frac{\\mu}{L}\\right) (f(w^k) - f^*). \\\\\n\\]\nUsing inequality recursively:\\[\nf(w^{k}) - f^* \\leq \\left(1 - \\frac{\\mu}{L}\\right) (f(w^{k-1}) - f^*). \\\\\nf(w^{k}) - f^* \\leq \\left(1 - \\frac{\\mu}{L}\\right) \\left(1 - \\frac{\\mu}{L}\\right) [f(w^{k-2} - f^*]. \\\\\nf(w^{k}) - f^* \\leq \\left(1 - \\frac{\\mu}{L}\\right)^3 [f(w^{k-3} - f^*]. \\\\\n...\\\\\nf(w^{k}) - f^* \\leq \\left(1 - \\frac{\\mu}{L}\\right)^k [f(w^{0} - f^*]. \\\\\n\\]\nsince \\(0 < \\mu \\le L\\), \\(\\left(1 - \\frac{\\mu}{L} \\right) \\le 0\\). implies \\(f(w^{k+1}) - f^* = O(\\rho^k) \\text{ } \\rho < 1\\).Using fact \\((1-x) \\ge e^{-x}\\) can rewrite :\\[\nf(w^{k}) - f^* \\leq exp\\left(k\\frac{\\mu}{L}\\right) [f(w^{0} - f^*]. \\\\\n\\]\nlinear convergence sometimes also called “exponential convergence”. \\(f(w^{k}) - f^* \\leq \\epsilon\\) \\(k \\ge \\frac{L}{\\mu} log (\\frac{f(w^0 - f^*)}{\\mu}) = O(log(1/\\epsilon))\\).PL satisfied many convex objective functions like least squares. PL satisfied many scenarios like neural network optimization. PL constant \\(\\mu\\) might bad functions. might hard show PL satisfiability many functions.Strong ConvexityA function \\(f\\) strong convex function:\\[\nf(w) - \\frac{\\mu}{2} \\|w\\|^2\n\\]\nalso convex function \\(\\mu > 0\\). informally, un-regularize \\(\\mu\\), function still convex. Strongly convex functions nice properties:unique global minimizing point \\(w^*\\) exists.\n\\(C^1\\) strongly convex functions satisfies PL inequality.\n\\(g(w) = f(Aw)\\) strongly convex \\(f\\) matrix \\(\\), \\(g\\) satisfies PL inequality.Strong Convexity Implies PL Inequality. Taylor’s theorem \\(C^2\\) functions:\\[\nf(v) = f(w) + \\nabla f(w)^\\top (v - w) + \\frac{1}{2} (v - w)^\\top \\nabla^2 f(u) (v - w).\n\\]strong convexity, \\(d^\\top \\nabla^2 f(u) d \\geq \\mu \\|d\\|^2 \\quad \\text{} d \\text{ } u.\\) :\\[\nf(v) \\geq f(w) + \\nabla f(w)^\\top (v - w) + \\frac{\\mu}{2} \\|v - w\\|^2\n\\]Treating right side function \\(v\\), get quadratic lower bound \\(f\\). minimizing respect \\(v\\) get:\\[\nf(w) - f^* \\leq \\frac{1}{2\\mu} \\|\\nabla f(w)\\|^2\\\\\n\\text{PL inequality.}\n\\]\nCombining Lipschitz Continuity Strong Convexity - Lipschitz continuity gradient gives guaranteed progress. Strong convexity functions gives maximum sub-optimality. Progress iteration least fixed fraction sub-optimality.Effect L2 Regularization Convergence Rate.convex loss \\(f\\), adding L2-regularization makes strongly-convex.\\[\nf(w) + \\frac{\\lambda}{2} ||w^2||, \\text{ } \\mu \\ge \\lambda\n\\]\nadding \\(L2\\) regulaizer improves rate sub-linear linear. go \\(O(\\frac{1}{\\epsilon})\\) \\(O(log(\\frac{1}{\\epsilon}))\\) guarantees unique minimizer.","code":""},{"path":"improving-gradient-descent.html","id":"improving-gradient-descent","chapter":"3 Improving Gradient Descent","heading":"3 Improving Gradient Descent","text":"","code":""},{"path":"improving-gradient-descent.html","id":"oracle-model-of-computation","chapter":"3 Improving Gradient Descent","heading":"3.1 Oracle Model of Computation","text":"analyze algorithms need two ingredients:\n1. Assumptions function like Lipschitz, PL, convexity, .\n2. Model computation, restricting algorithm can .Standard model computation first-order oracle model:iteration algorithm chooses point \\(w^k\\).algorithm gets \\(f(w^k)\\) \\(\\nabla f(w^k)\\).analyze many iterations needed make quantity small.\nUsually \\(\\| \\nabla f(w^k) \\|\\) \\(f(w^k) - f^*\\) \\(\\| w^k - w^* \\|\\).Given assumptions oracle model, can prove upper bounds iteration complexity specific algorithms prove lower bounds iteration complexity across algorithms.first-order oracle model algorithm often unrestricted,\ncan learn function evaluations chosen \\(w^k\\).\nOften prove lower bounds designing “worst function” assumptions.\nshow can slowly discover minimum location oracle.","code":""},{"path":"improving-gradient-descent.html","id":"heavy-ball-method","chapter":"3 Improving Gradient Descent","heading":"3.2 Heavy Ball Method","text":"\\[\nw^{k+1} = w^{k} - \\alpha_k \\nabla f(w^k) + \\beta_k (w^k - w^{k-1}) \\\\\n\\]\nadds momentum term gradient descent iteration \\(k > 1\\). Informally term makes us go previous direction. \\(\\beta_k \\[0, 1)\\)Heavy-ball method can increase function “overshoot” optimum. reach optima quicker.Considering heavy-ball method choices:\n\\[\n\\alpha_k = \\frac{4}{(\\sqrt{L} + \\sqrt{\\mu})^2}, \\quad \\beta_k = \\left( \\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}} \\right)^2.\n\\]choices heavy-ball method :\n\\[\n\\|w_k - w^*\\| \\leq \\left( \\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}} + \\epsilon_k \\right)^k \\|w_0 - w^*\\|,\n\\]\n\\(\\epsilon_k \\0\\).Instead directly bounding \\(\\|w_k - x^*\\|\\), proof bounds \\(\\|w_k - w^*\\|^2 + \\|w_{k-1} - w^*\\|^2\\). Shows function “bigger” converging right rate.optimal dimension-independent rate first-order oracle model :\n\\[\n\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}.\n\\]choice heavy-ball method close optimal.","code":""},{"path":"improving-gradient-descent.html","id":"conjugate-gradient-heavy-ball-with-optimal-parameters","chapter":"3 Improving Gradient Descent","heading":"3.3 Conjugate Gradient: Heavy-Ball with Optimal Parameters","text":"quadratics, optimize \\(\\alpha_k\\) \\(\\beta_k\\) iteration. iteration, choose \\(\\alpha_k\\) \\(\\beta_k\\) maximally decrease \\(f\\). “Plane search” (“subspace optimization”) along two directions instead “line search”. “optimal heavy-ball” method called conjugate gradient (CG) method:\\(\\alpha_k = \\frac{\\nabla f(w_k)^T d_k}{d_k^T d_k}\\) (step size gradient direction)\\(\\beta_k = \\frac{\\alpha_k \\beta_{\\hat{k}-1}}{\\alpha_{k-1}}\\) (momentum parameter, \\(\\beta_0 = 0\\))\\(w_{k+1} = w_k - \\alpha_k \\nabla f(w_k) + \\beta_k (w_k - w_{k-1})\\) (heavy-ball update)\\(\\beta_{\\hat{k}} = \\frac{\\nabla f(w_{k-1})^T d_{k-1}}{d_{k-1}^T d_{k-1}}\\) (search direction, \\(d_0 = -\\nabla f(w_0)\\))Gradients iterations orthogonal, \\(\\nabla f(w_k)^T \\nabla f(w_{k-1}) = 0\\).Achieves optimal \\(\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}\\) dimension-independent rate.can show conjugate gradient minimizes d-dimensional quadratic \\(d\\) steps. Tends happen computers due floating point issues. Note conjugate gradient need know \\(L\\) \\(µ\\).","code":""},{"path":"improving-gradient-descent.html","id":"nesterov-accelerated-gradient","chapter":"3 Improving Gradient Descent","heading":"3.4 Nesterov Accelerated Gradient","text":"call first-order method accelerated either \\(O(1/k^2)\\) rate convex functions linear rate depending \\(\\sqrt{L}\\) \\(\\sqrt{\\mu}\\) strongly-convex functions\\[\nw^{k+1} = w^{k} - \\alpha_k \\nabla f(v^k) \\\\\nv^{k+1} = w^{k+1} + \\beta_k (w^{k+1} - w^{k}) \\\\\n\\]\nNesterov’s Acceleration computes gradient applying momentum.Consider optimizing one-dimensional convex function. sign gradient stays , Nesterov’s algorithm speeds heavy-ball. sign gradient changes (overshooting minima), “slows ” faster.Nesterov’s method typically analyzed \\(\\alpha_k = \\frac{1}{L}\\).convex functions, accelerated rate can achieved - \\(\\beta_k = \\frac{k - 1}{k + 2}\\), momentum converges 1.strongly-convex functions, acceleration can achieved constant - \\(\\beta_k = \\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}\\), heavy-ball method.Notice need different parameters different problems. Using momentum converges 1 strongly-convex slow. Unlike gradient descent adapts problem standard choices. Using \\(\\alpha_k = \\frac{1}{L}\\) maintains rate convex, strongly-convex, non-convex.practice, can maintain accelerated rate without knowing \\(L\\).Start guess \\(\\hat{L}\\)Start guess \\(\\hat{L}\\)Given momentum step \\(v_k\\), test inequality\n\\(f(w_{k+1}) \\leq f(v_k) - \\frac{1}{2\\hat{L}} \\| \\nabla f(v_k) \\|^2\\),\ndouble \\(\\hat{L}\\) satisfied.Given momentum step \\(v_k\\), test inequality\n\\(f(w_{k+1}) \\leq f(v_k) - \\frac{1}{2\\hat{L}} \\| \\nabla f(v_k) \\|^2\\),\ndouble \\(\\hat{L}\\) satisfied.gradient descent, can work much better knowing \\(L\\).Nesterov’s method often non-monotonic. always $ f(w_{k+1}) < f(w_k) $. momentum, necessarily bad thing.TODO 2nd derivative method","code":""},{"path":"coordinate-optimization.html","id":"coordinate-optimization","chapter":"4 Coordinate Optimization","heading":"4 Coordinate Optimization","text":"","code":""},{"path":"coordinate-optimization.html","id":"definition-and-examples","chapter":"4 Coordinate Optimization","heading":"4.1 Definition and examples","text":"iteration coordinate optimization, one variable (coordinate) updated. updating one variable time, optimization process becomes simpler efficient.iteration \\(k\\) select variable \\(j_k\\) set\n\\[\nw_{k+1}^{j_k} = w_k^{j_k} - \\alpha_k \\nabla_{j_k} f(w_k)\n\\]\ngradient descent step one coordinate \\(j_k\\) (\\(w_j\\) stay ).Theoretically, coordinate descent provably bad algorithm. convergence rate slower gradient descent. iteration cost can similar gradient descent. Computing 1 partial derivative may cost computing gradient.widely-used practice. Nothing works better certain problems. Certain fields think “ultimate” algorithm.Global convergence rate can showed randomized coordinate selection. Coordinate descent faster gradient descent iterations \\(d\\) times cheaper. Sometimes called coordinate-friendly structures.functions coordinate descent \\(d\\) times faster gradient descent?simplest example separable functions:\n\\[\nf(w) = \\sum_{j=1}^{d} f_j(w_j)\n\\]\n\\(f\\) sum \\(f_j\\) applied \\(w_j\\), like \\(f(w) = \\frac{\\lambda}{2} \\|w\\|^2 = \\sum_{j=1}^{d} \\frac{\\lambda}{2} w_j^2\\).Gradient descent costs \\(O(d)\\) compute \\(f'_j(w_k^j)\\).\nCoordinate descent costs \\(O(1)\\) compute one \\(f'_jk(w_k^{jk})\\).separable functions use coordinate optimization. variables \\(w_j\\) “separate” effects, can minimized independently.interesting example pairwise-separable functions:\n\\[\nf(w) = \\sum_{=1}^{d} \\sum_{j=1}^{d} f_{ij}(w_i, w_j),\n\\]\ndepend function pair variables. includes quadratic functions. example label propagation semi-supervised learning. \\(f_{ij}\\) measures similar labels neighbors.double sum \\(O(d^2)\\) terms. Gradient descent needs compute gradient terms.\n\\(w_j\\) appears \\(O(d)\\) terms. Coordinate optimization needs use terms.label propagation example looks bit like :\\[\nf(w) = \\sum_{j=1}^{d} f_j(w_j) + \\sum_{(,j) \\E} f_{ij}(w_i, w_j),\n\\]\\(E\\) set \\((, j)\\) edges graph. Adding separable function doesn’t change costs. just combine \\(f_j\\) one \\(f_{ij}\\). Restricting \\((, j)\\) \\(E\\) makes gradient descent cheaper. Now costs \\(O(|E|)\\) compute gradient. Coordinate descent also cost \\(O(|E|)\\) degree \\(j_k\\) \\(O(|E|)\\). Coordinate descent still \\(d\\) times faster expectation randomly update coordinates.Another coordinate-friendly structure linear compositions:\\[\nf(w) = g(Aw), \\text{ } n \\times d \\text{ matrix sooth function } g\n\\]\nstill coordinate friendly add separable function like L2-regularizer.\n\\[\nf(w) = g(Aw) + \\sum_{j=1}^{d} f_j(w_j),\n\\]main idea can track \\(Aw_k\\) go cost \\(O(n)\\) instead \\(O(nd)\\).linear compositions problems, partial derivatives iteration \\(k\\) form:\n\\[\n\\nabla_j f(w_k) = a_j^T g'(Aw_k),\n\\]\n\\(a_j\\) column \\(j\\) \\(\\).\\(Aw^k\\), costs \\(O(n)\\) instead \\(O(nd)\\) full gradient. (Assuming \\(g'\\) costs \\(O(n)\\))\ncan track product \\(Aw_k\\) go \\(O(n)\\) cost:\\[\nAw^{k+1} = (w^k + \\gamma_k e_{jk}) = Aw^k + \\gamma_k a_j.\n\\]\nallows computing partial derivatives implementing line-search steps \\(O(n)\\).Neural networks usually coordinate friendly. need something like “number units first hidden layer tiny”. Updating just one parameter can lead complex non-local effects overall function network.","code":""},{"path":"coordinate-optimization.html","id":"analyzing-coordinate-descent","chapter":"4 Coordinate Optimization","heading":"4.2 Analyzing Coordinate Descent","text":"analyze coordinate descent, can write \n\\[\nw_{k+1} = w_k - \\alpha_k \\nabla_{jk} f(w_k) e_{jk},\n\\]\n“elementary vector” \\(e_j\\) zero every position except \\(j\\),\n\\[\ne_j^T =\n\\begin{bmatrix}\n0 & \\cdots & 1 & \\cdots & 0\n\\end{bmatrix}\n\\]\nusually assume \\(\\nabla_j f\\) \\(L\\)-Lipshitz (“coordinate-wise Lipschitz”),\n\\[\n|\\nabla_j f(w + \\gamma e_j) - \\nabla_j f(w)| \\leq L|\\gamma|\n\\]\n\\(C^2\\) functions equivalent \\(|\\nabla_{jj}^2 f(w)| \\leq L\\) \\(j\\).stronger assumption gradient descent - gradient \\(L\\)-Lipschitz also coordinate-wise \\(L\\)-Lipschitz.Coordinate-wise Lipschitz assumption implies descent lemma coordinate-wise:\n\\[\nf(w^{k+1}) \\leq f(w^k) + \\nabla_j f(w^k)(w^{k+1} - w^k)_j + \\frac{L}{2}(w^{k+1} - w^k)^2_j\n\\]\n\\(w^{k+1}\\) \\(w_k\\) differ coordinate \\(j\\). \\(\\alpha_k = \\frac{1}{L}\\) (simplicity), plugging \\((w_{k+1} - w_k) = -\\frac{1}{L}e_{jk}\\nabla_{jk} f(w_k)\\) gives\\[\nf(w^{k+1}) \\leq f(w^k) - \\frac{1}{2L}|\\nabla_{jk} f(w^k)|^2,\n\\]\nprogress bound based updating coordinate \\(j_k\\).","code":""},{"path":"coordinate-optimization.html","id":"randomized-cd-progress","chapter":"4 Coordinate Optimization","heading":"4.3 Randomized CD Progress","text":"progress randomized coordinate descent depends random selection \\(j_k\\). expected progress :\\[\n\\mathbb{E}[f(w^{k+1})] \\leq \\mathbb{E}\\left[ f(w^k) - \\frac{1}{2L}|\\nabla_{jk} f(w^k)|^2 \\right] \\text{ expected wrt } j_k \\text{ given }w^k\\\\\n= \\mathbb{E}[f(w^k)] - \\frac{1}{2L}\\mathbb{E}[|\\nabla_{jk} f(w^k)|^2]\\\\\n= f(w^k) - \\frac{1}{2L} \\sum_{j=1}^{d} \\mathbb{P}(j_k = j)|\\nabla_j f(w^k)|^2\\\\\n\\]\nExpectation conditioned steps time \\(k\\).Let’s choose \\(j_k\\) uniformly random bound, \\(p(j_k = j) = \\frac{1}{d}\\).\\[\nE[f(w_{k+1})] \\leq f(w_k) - \\frac{1}{2L} \\sum_{j=1}^{d} \\frac{1}{d} |\\nabla_j f(w_k)|^2\\\\\n= f(w_k) - \\frac{1}{2dL} \\sum_{j=1}^{d} |\\nabla_j f(w_k)|^2\\\\\n= f(w_k) - \\frac{1}{2dL}k\\|\\nabla f(w_k)\\|^2\n\\]Random-Shuffling Coordinate SelectionAn alternative random selection \\(j_k\\) cyclic selection:\n\\[\nj_1 = 1, j_2 = 2, \\ldots, j_d = d \\\\\nj_{d+1} = 1, j_{d+2} = 2, \\ldots, j_{2d} = d \\\\\nj_{2d+1} = 1, j_{2d+2} = 2, \\ldots, j_{3d} = d \\\\\n\\]\nCyclic often outperforms random practice, worse theory.problems, bad ordering leads provably-bad performance cyclic.Hybrid cyclic random using random shuffling:Choose random permutation \\(r\\) set \\(j_1 = r[1], j_2 = r[2], \\ldots, j_d = r[d]\\).Choose random permutation \\(r\\) set \\(j_{d+1} = r[1], j_{d+2} = r[2], \\ldots, j_{2d} = r[d]\\).Choose random permutation \\(r\\) set \\(j_{2d+1} = r[1], j_{2d+2} = r[2], \\ldots, j_{3d} = r[d]\\).Recent work shows fixes cyclic coordinate descent settings. Conjectured random shuffling faster cyclic random.","code":""},{"path":"coordinate-optimization.html","id":"gauss-southwell-greedy-coordinate-descent","chapter":"4 Coordinate Optimization","heading":"4.4 Gauss-Southwell: Greedy Coordinate Descent","text":"Instead cyclic random, also greedy coordinate selection methods.classic greedy method Gauss-Southwell rule,\\[\nj_k \\\\arg\\max_j \\{|\\nabla_j f(w_k)|\\}\n\\]chooses coordinate largest directional derivative.leads progress bound :\\[\nf(w_{k+1}) \\leq f(w_k) - \\frac{1}{2L} \\|\\nabla f(w_k)\\|^2_\\infty\n\\]similar gradient descent different norm. Unlike random coordinate descent, dimension independent. PL functions leads rate :\\[\nf(w_k) - f^* \\leq \\left( 1 - \\frac{\\mu_1}{L} \\right)^k [f(w_0) - f^*]\n\\]\\(\\mu_1\\) PL constant \\(\\infty\\)-norm\\[\n\\mu_1 [f(w) - f^*] \\leq \\frac{1}{2} \\|\\nabla f(w)\\|^2_\\infty\n\\]faster random \\(\\mu_d \\leq \\mu_1 \\leq \\mu\\) (norm equivalences). \\(\\mu_1\\)-PL condition implied strong-convexity 1-norm.Convergence Rate\\[\nf(w_{k+1}) \\leq f(w_k) - \\frac{1}{2L_k} \\|\\nabla f(w_k)\\|_\\infty^2,\\\\\n\\text{PL gives us:}\\\\\nf(w_k) - f^* \\leq \\left( 1 - \\frac{\\mu_1}{L} \\right)^k [f(w_0) - f^*] \\\\\n\\text{} \\mu_1 \\text{ PL constant maximum norm:}\\\\\n\\mu_1[f(w) - f^*] \\leq \\frac{1}{2} \\| \\nabla f(w) \\|_\\infty^2\n\\]Lipschitz Sampling","code":""},{"path":"stochastic-gradient-methods.html","id":"stochastic-gradient-methods","chapter":"5 Stochastic Gradient Methods","heading":"5 Stochastic Gradient Methods","text":"","code":""},{"path":"stochastic-gradient-methods.html","id":"introduction","chapter":"5 Stochastic Gradient Methods","heading":"5.1 Introduction","text":"Stochastic gradient descent primarily used calculating full gradient \\(t\\) iterations costly. machine learning prespective, instead calculating gradient using entire dataset, calculate using randomly picked data point (small batches data points) instead.couple data points might point wrong direction. average move towards minima. erractic nature useful optimizing non-convex landscapes like neural network stochasic nature can allow escape saddle points local optimia reach better deeper optima.Random selection \\(i_k\\) \\(\\{1, 2, \\ldots, n\\}\\).\n\\[\nw^{k+1} = w^k - \\alpha_k \\nabla f_{i_k}(w^k).\n\\]\n\\(p(i_k = ) = 1/n\\), stochastic gradient unbiased estimate gradient:\n\\[\n\\mathbb{E}[\\nabla f_{i_k}(w)] = \\frac{1}{n} \\sum_{=1}^{n} p(i_k = ) \\nabla f_i(w) = \\frac{1}{n} \\sum_{=1}^{n} \\frac{1}{n} \\nabla f_i(w) = \\frac{1}{n} \\sum_{=1}^{n} \\nabla f_i(w) = \\nabla f(w).\n\\]\nIteration cost independent \\(n\\). Convergence requires \\(\\alpha_k \\0\\). Stochastic low iteration cost slow convergence rate.","code":""},{"path":"stochastic-gradient-methods.html","id":"progress-bound-for-sgd","chapter":"5 Stochastic Gradient Methods","heading":"5.2 Progress Bound for SGD","text":"stochastic gradient descent (SGD) update \n\\[ w^{k+1} = w^k - \\alpha_k \\nabla f_{i_k}(w^k) \\]\nRecall descent lemma applied \\(w_{k+1}\\) \\(w_k\\),\n\\[ f(w_{k+1}) \\leq f(w_k) + \\nabla f(w_k)^\\top (w_{k+1} - w_k) + \\frac{L}{2} \\| w_{k+1} - w_k \\|_2^2\\]\nPlugging SGD iteration \\((w_{k+1} - w_k) = -\\alpha_k \\nabla f_{i_k}(w_k)\\) gives\n\\[f(w^{k+1}) \\leq f(w^k) - \\alpha_k \\nabla f(w^k)^\\top \\nabla f_{i_k}(w^k) + \\alpha_k^2 \\frac{ L}{2} \\|\\nabla f_{i_k}(w^k)\\|^2 \\]far choice \\(\\alpha_k\\) \\(i_k\\) \n\\[\nf(w_{k+1}) \\leq f(w_k) - \\alpha_k \\nabla f(w_k)^\\top \\nabla f_{i_k}(w_k) + \\alpha_k^2 L_k^2 \\|\\nabla f_{i_k}(w_k)\\|^2.\n\\]\nLet’s take expectation assume \\(\\alpha_k\\) depend \\(i_k\\),\n\\[\n\\mathbb{E}[f(w^{k+1})] \\leq \\mathbb{E}[f(w^k) - \\alpha_k \\nabla f(w^k)^\\top \\nabla f_{i_k}(w^k) + \\alpha_k^2 L_k^2 \\|\\nabla f_{i_k}(w^k)\\|^2] \\\\ = f(w^k) - \\alpha_k \\nabla f(w^k)^\\top \\mathbb{E}[\\nabla f_{i_k}(w^k)] + \\alpha_k^2 L_k^2 \\mathbb{E}[\\|\\nabla f_{i_k}(w^k)\\|^2]\n\\]\nuniform sampling \\(\\mathbb{E}[\\nabla f_{i_k}(w_k)] = \\nabla f(w_k)\\) (unbiased) gives\n\\[\n\\mathbb{E}[f(w^{k+1})] \\leq f(w^k) - \\alpha_k k \\|\\nabla f(w^k)\\|^2 + \\alpha_k^2 L_k^2 \\mathbb{E}[k\\|\\nabla f_{i_k}(w^k)\\|^2]\n\\]Choosing \\(\\alpha_k = 1/L\\) might small enough. \\(\\alpha_k\\) small \\(\\alpha_k << \\alpha_k^2\\). \\(\\alpha_k\\) controls much move towards solution iteration. \\(\\alpha_k^2\\) controls much stochasic nature moves us away solution.Analyzing SGD assuming $ f $ bounded . non-convex. bound effect noise, assume $ $ :\\[ \\mathbb{E}[k\\|\\nabla f_i(w)\\|^2] \\leq \\sigma^2 \\].implies gradients bounded, hold PL functions. Using noise assumption inside progress bound,\n\\[\n\\mathbb{E}[f(w^{k+1})] \\leq f(w^k) - \\alpha^k k\\|\\nabla f(w^k)\\|^2 + \\frac{\\alpha_k^2 L\\sigma^2}{2}\n\\]\nRearranging get gradient norm left side:\n\\[\n\\alpha_k k\\|\\nabla f(w^k)\\|^2 \\leq f(w^k) - \\mathbb{E}[f(w^{k+1})] + \\frac{\\alpha_k^2 L\\sigma^2}{2}.\n\\]\nSumming using iterated expectation get:\n\\[\n\\sum_{k=1}^{t} \\alpha_{k-1} \\mathbb{E}_k\\|\\nabla f(w^{k-1})\\|^2 \\leq \\sum_{k=1}^{t} [\\mathbb{E}f(w^{k-1}) - \\mathbb{E}f(w^k)] + \\sum_{k=1}^{t} \\alpha_{k-1}^2 L\\sigma^2\n\\]\nApplying operations gives\n\\[\n\\min_{k=0,1,...,t-1} \\{\\mathbb{E}_k\\|\\nabla f(w^k)\\|^2\\} \\cdot \\sum_{k=0}^{t-1} \\alpha_k \\leq f(w^0) - \\mathbb{E}_f(w^t) + \\frac{L\\sigma^2}{2} \\sum_{k=0}^{t-1} \\alpha_k^2.\n\\]\nUsing \\(\\mathbb{E}_f(w_k) \\geq f^*\\) dividing sides \\(\\sum_{k} \\alpha_{k-1}\\) gives\n\\[\n\\min_{k=0,1,...,t-1} \\{\\mathbb{E}_k\\|\\nabla f(w^k)\\|^2\\} \\leq \\frac{f(w^0) - f^*}{\\sum_{k} \\alpha_{k} (1 - \\alpha_{k} L/2)} + \\frac{L\\sigma^2}{2} \\frac{\\sum_{k=0}^{t-1} \\alpha_k^2}{\\sum_{k} \\alpha_{k-1}}.\n\\]","code":""},{"path":"stochastic-gradient-methods.html","id":"convergence-of-sgd-for-pl-functions","chapter":"5 Stochastic Gradient Methods","heading":"5.3 Convergence of SGD for PL functions","text":"\\[\n\\text{Starting SGD progress bound -} \\\\\nE[f(w_{k+1})] \\leq f(w_k) - \\alpha_k \\| \\nabla f(w_k) \\|^2 + \\frac{\\alpha_k^2 L}{2} E[\\|\\nabla f_i(w_k) \\|^2] \\\\\n\\text{Bounding PL inequality (} \\| \\nabla f(w_k) \\|^2 \\geq 2\\mu(f(w_k) - f^*) \\text{)} \\\\\nE[f(w_{k+1})] \\leq f(w_k) - \\alpha_k 2\\mu (f(w_k) - f^*) + \\frac{\\alpha_k^2 L \\sigma^2}{2}. \\\\\nE[f(w_{k+1})] - f^* \\leq (1 - 2\\alpha_k \\mu)(f(w_k) - f^*) + \\frac{\\alpha_k^2 L \\sigma^2}{2}. \\\\\n\\leq (1 - 2\\alpha\\mu) \\left((1 - 2\\alpha\\mu)(f(w_{k-1}) - f^*) + \\frac{\\alpha^2 L \\sigma^2}{2}\\right) + \\frac{\\alpha^2 L \\sigma^2}{2} \\\\\n= (1 - 2\\alpha\\mu)^2 (f(w_{k-1}) - f^*) + \\frac{\\alpha^2 L \\sigma^2}{2} (1 + (1 - 2\\alpha\\mu)) \\\\\n\\text{Applying recursively } k \\text{ } 0, \\text{ get -} \\\\\nE[f(w_k)] - f^* \\leq (1 - 2\\alpha\\mu)^k (f(w_0) - f^*) + \\frac{\\alpha^2 L \\sigma^2}{2} \\sum_{t=0}^k (1 - 2\\alpha\\mu)^t \\\\\n\\sum_{t=0}^k (1 - 2\\alpha\\mu)^t < \\sum_{t=0}^\\infty (1 - 2\\alpha\\mu)^t = \\frac{1}{2\\alpha\\mu} \\text{ ( geometric series)}.\n\\]\nConvergence rate SGD constant step size PL functions - \\\\[\nE[f(w_k) - f^*] \\leq (1 - 2\\alpha\\mu)^k (f(w_0) - f^*) + \\frac{\\alpha \\sigma^2 L}{4\\mu}\n\\]Thie first term linear convergence 2nd term drop 0. leads erratic behavior making good progress. aligns stochastic nature SGD. probability random data point pointing , least part, toward minima higher away . SGD gets confused close minima. happens, can half step size reducing space optimization erratic. Halving \\(\\alpha\\) divides bound distance \\(f^*\\) half.","code":""},{"path":"stochastic-gradient-methods.html","id":"when-to-stop","chapter":"5 Stochastic Gradient Methods","heading":"5.3.1 When to stop?","text":"Gradient Descent stopped making enough progress gradient close zero. EGD, gradients guaranteed go zero see full gradient. can instead every day iterations measure validation area validation starts go , stop also called early stopping","code":""},{"path":"stochastic-gradient-methods.html","id":"mini-batches-and-batch-growing","chapter":"5 Stochastic Gradient Methods","heading":"5.4 Mini Batches and Batch Growing","text":"Deterministic gradient descent uses \\(n\\) samples calculate gradient.\n\\[\n\\nabla f(w_k) = \\frac{1}{n} \\sum_{=1}^n \\nabla f_i(w_k)\\\\\n\\]Stochastic gradient descent approximates gradient 1 sample.\n\\[\n\\nabla f(w_k) \\approx \\nabla f_{i_k}(w_k)\\\\\n\\]\ncommon variant use \\(m\\) samples mini-batch \\(B_k\\)\n\\[\n\\nabla f(w_k) \\approx \\frac{1}{m} \\sum_{\\B_k} \\nabla f_i(w_k)\n\\]\nUsing batch data points data reduces probability gradient points “worse” direction full gradient point calculate . Mini-batches useful parallelization. example, 16 cores set \\(m = 16\\) compute 16 gradients .mini-batch gradient, expectation, full gradient way SGD.","code":""},{"path":"stochastic-gradient-methods.html","id":"variation-in-mini-batch-approximation","chapter":"5 Stochastic Gradient Methods","heading":"5.5 Variation in Mini-Batch Approximation","text":"analyze variation gradients, use variance-like identity:\nrandom variable g unbiased approximation vector μ, \\[\nE[\\|g - \\mu\\|^2] = E[\\|g\\|^2 - 2g^T \\mu + \\|\\mu\\|^2] \\quad \\\\\n= E[\\|g\\|^2] - 2E[g]^T \\mu + \\|\\mu\\|^2 \\quad \\\\\n= E[\\|g\\|^2] - 2\\mu^T \\mu + \\|\\mu\\|^2 \\quad (\\text{unbiased}) \\\\\n= E[\\|g\\|^2] - \\|\\mu\\|^2.\n\\]Expectation inner product independent samples -\\[\nE[\\nabla f_i(w)^T \\nabla f_j(w)] = \\sum_{=1}^n \\sum_{j=1}^n \\frac{1}{n^2} \\nabla f_i(w)^T \\nabla f_j(w) \\\\\n= \\frac{1}{n} \\sum_{=1}^n \\nabla f_i(w)^T \\left(\\frac{1}{n} \\sum_{j=1}^n \\nabla f_j(w)\\right) \\\\\n= \\frac{1}{n} \\sum_{=1}^n \\nabla f_i(w)^T \\nabla f(w) \\quad (\\text{gradient } f) \\\\\n= \\left(\\frac{1}{n} \\sum_{=1}^n \\nabla f_i(w)\\right)^T \\nabla f(w)  \\\\\n= \\nabla f(w)^T \\nabla f(w) = \\|\\nabla f(w)\\|^2 .\n\\]Let us see error goes use points batch.Let \\(g_2(w) = \\frac{1}{2} (\\nabla f_i(w) + \\nabla f_j(w))\\) mini-batch approximation 2 samples.\n\\[\nE[\\|g_2(w) - \\nabla f(w)\\|^2] = E\\left[\\left\\|\\frac{1}{2} (\\nabla f_i(w) + \\nabla f_j(w))\\right\\|^2\\right] - \\|\\nabla f(w)\\|^2  \\\\\n= \\frac{1}{4} E[\\|\\nabla f_i(w)\\|^2] + \\frac{1}{2} E[\\nabla f_i(w)^T \\nabla f_j(w)] + \\frac{1}{4} E[\\|\\nabla f_j(w)\\|^2] - \\|\\nabla f(w)\\|^2 \\quad  \\\\\n= \\frac{1}{2} E[\\|\\nabla f_i(w)\\|^2] + \\frac{1}{2} E[\\nabla f_i(w)^T \\nabla f_j(w)] - \\|\\nabla f(w)\\|^2 \\quad (E[\\nabla f_i] = E[\\nabla f_j]) \\\\\n= \\frac{1}{2} E[\\|\\nabla f_i(w)\\|^2] + \\frac{1}{2} \\|\\nabla f(w)\\|^2 - \\|\\nabla f(w)\\|^2 \\quad (E[\\nabla f_i \\nabla f_j] = \\|\\nabla f(w)\\|^2) \\\\\n= \\frac{1}{2} E[\\|\\nabla f_i(w)\\|^2] - \\frac{1}{2} \\|\\nabla f(w)\\|^2 \\\\\n= \\frac{1}{2} \\left(E[\\|\\nabla f_i(w)\\|^2] - \\|\\nabla f(w)\\|^2\\right)  \\\\\n= \\frac{1}{2} E[\\|\\nabla f_i(w) - \\nabla f(w)\\|^2] \\quad  \\\\\n= \\frac{\\sigma(w)^2}{2} \\quad (\\sigma^2 \\text{ 1-sample variation}) \\\\\n\\]SGD error \\(E[\\|e_k\\|^2]\\) cut half compared using 1 sample.\\(m\\) samples can show -\\[\nE[\\|e_k\\|^2] = \\frac{\\sigma(w^k)^2}{m}\n\\]\\(\\sigma(w^k)^2\\) variance individual gradients \\(w^k\\). larger batch size size \\(m\\), effect stochasticity reduced \\(m\\). larger batch size size \\(m\\), cause use step size \\(m\\) times larger. Doubling batch size effect halving step size.","code":""},{"path":"overparameterization.html","id":"overparameterization","chapter":"6 Overparameterization","heading":"6 Overparameterization","text":"","code":""},{"path":"overparameterization.html","id":"overparameterization-and-sgd","chapter":"6 Overparameterization","heading":"6.1 Overparameterization and SGD","text":"Overparameterization means model can fit entire training data. Modern machine learning models like neural networks tend well overparameterized settings. Typically, complicated models, tend overfit.overparameterized regime \\(\\nabla f_i (w_*) = 0\\) \\(\\). variance 0 minimizers SGD converges small enough step size.One way characterize -parameterization: strong growth condition (SGC) -\\[\nE[\\|\\nabla f_i(w)\\|^2] \\leq \\rho \\|\\nabla f(w)\\|^2, \\\\\n\\text{implies interpolation property } \\nabla f(w) = 0 \\Rightarrow \\nabla f_i(w) = 0 \\text{ } .\n\\]\nBound Error SGC -SGC, SGD error bounded full gradient size\\[\nE[\\|e_k\\|^2] = E[\\|\\nabla f_i(w_k) - \\nabla f(w_k)\\|^2] \\\\\n= \\|\\nabla f(w_k)\\|^2 - \\|\\nabla f_i(w_k)\\|^2 \\\\\n= \\frac{1}{n} \\sum_{=1}^n \\|\\nabla f_i(w_k)\\|^2 - \\|\\nabla f_i(w_k)\\|^2 \\\\\n\\leq \\frac{1}{n} \\sum_{=1}^n \\rho \\|\\nabla f(w_k)\\|^2 - \\|\\nabla f(w_k)\\|^2 \\quad (\\text{using SGC}) \\\\\n= (\\rho - 1) \\|\\nabla f(w_k)\\|^2 \\\\\n\\text{SGC, need assumption like } E[\\|e_k\\|^2] \\leq \\sigma^2.\n\\]Using SGD descent lemma \\(\\alpha_k = \\frac{1}{L\\rho}\\) SGC obtain -\\[\nE[f(w_{k+1})] \\leq f(w_k) - \\frac{1}{2L\\rho} \\|\\nabla f(w_k)\\|^2, \\\\\n\\]function decrease deterministic gradient descent factor \\(\\rho\\). step size \\(\\alpha < \\frac{2}{L}\\rho\\) guarantees descent.","code":""},{"path":"overparameterization.html","id":"faster-sgd-for-overparameterized-models","chapter":"6 Overparameterization","heading":"6.2 Faster SGD for Overparameterized Models","text":"-parameterization leads faster convergence rates SGD. now exist methods go faster -parameterized setting. -parameterization, can also use SGD line-search.setting SGD step size go works broadly -“Update step size based simple statistics”.“Update step size based simple statistics”.“gradient descent step size”.“gradient descent step size”.“Use line-search/trust-region based mini-batch”.“Use line-search/trust-region based mini-batch”.methods problems like -Introduces new hyper-parameters just hard tune step size.Introduces new hyper-parameters just hard tune step size.converge theoretically.converge theoretically.Converges theoretically, works badly practice.Converges theoretically, works badly practice.Needs assume \\(\\sigma_k\\) goes 0 workNeeds assume \\(\\sigma_k\\) goes 0 work","code":""},{"path":"overparameterization.html","id":"stochastic-line-search","chapter":"6 Overparameterization","heading":"6.3 Stochastic Line Search","text":"Armijo line-search mini-batch selects step size satisfying -\\[\nf_i(w_k - \\alpha_k \\nabla f_i(w_k)) \\leq f_i(w_k) - c \\alpha_k \\|\\nabla f_i(w_k)\\|^2,\n\\text{constant } c > 0. \\\\\n\\text{}\n\\]Without interpolation work (satisfied steps large). interpolation, can guarantee sufficient progress towards solution.Line search works well model overparameterized close overparameterized works horribly far overparameterized.","code":""},{"path":"variations-on-sgd.html","id":"variations-on-sgd","chapter":"7 Variations on SGD","heading":"7 Variations on SGD","text":"","code":""},{"path":"variations-on-sgd.html","id":"stochastic-average-gradient","chapter":"7 Variations on SGD","heading":"7.1 Stochastic Average Gradient","text":"Growing \\(|B_k|\\) eventually requires \\(O(n)\\) iteration cost.Let’s view gradient descent performing iteration:\\[\nw_{k+1} = w_k - \\frac{\\alpha_k}{n} \\sum_{=1}^n v_k^, \\\\\n\\text{step set } v_k^= \\nabla f_i(w_k) \\text{ } . \\\\\n\\]SAG method - set \\(v_k^{i_k} = \\nabla f_{i_k}(w_k)\\) randomly-chosen \\(i_k\\). \\(v_k^\\) kept previous value.can think SAG memory -\\[\n\\text{} \\\\\n\\begin{pmatrix}\n...& v_1 &...\\\\\n...& v_2 &...\\\\\n...& \\vdots &... \\\\\n...& v_n &...\n\\end{pmatrix} \\\\\n\\]\n\\(v_k^\\) gradient \\(\\nabla f_i(w_k)\\) last \\(k\\) \\(\\) selected.iteration -Randomly select index \\(i_k\\) \\(\\{1, 2, ..., n\\}\\).Randomly select index \\(i_k\\) \\(\\{1, 2, ..., n\\}\\).Compute gradient \\(\\nabla f_{i_k}(w_k)\\) using data point corresponding index \\(i_k\\).Compute gradient \\(\\nabla f_{i_k}(w_k)\\) using data point corresponding index \\(i_k\\).Update corresponding entry \\(v_k\\) \\(\\nabla f_{i_k}(w_k)\\).Update corresponding entry \\(v_k\\) \\(\\nabla f_{i_k}(w_k)\\).Update parameter vector \\(w_{k+1}\\) using average gradients stored \\(v_k\\).Update parameter vector \\(w_{k+1}\\) using average gradients stored \\(v_k\\).Unlike batches, use gradient every example. gradients date.","code":""},{"path":"variations-on-sgd.html","id":"variance-reduced-stochastic-gradient","chapter":"7 Variations on SGD","heading":"7.2 Variance Reduced Stochastic Gradient","text":"SVRG iteration -\\[\nw_{k+1} = w_k - \\alpha_k(\\nabla f_{i_k}(w_k) - \\nabla f_{i_k}(v_k) + \\nabla f(v_k)) \\\\\n\\]\nUnlike SAG, gives unbiased gradient approximation.\\[\nE[g_k] = \\nabla f(w_k) - E[\\nabla f_{i_k}(v_k)] + \\nabla f(v_k) \\underbrace{= 0}. \\\\\n\\]\ncan show gradient approximation goes 0 \\(w_k\\) \\(v_k\\) approaches \\(w^*\\).\n\\[\nE[\\|g_k\\|^2] \\leq 4L(f(w_k) - f^*) + 4L(f(v_k) - f^*).\n\\]idea behind SVRG compute full gradient sometimes leverages local information global information reduce variance updates.iterations “cheap” iterations \\(v^k = v^{k+1}\\)require 2 gradient evaluations saved \\(\\nabla f (v^{k-1})\\).occasionally compute full gradient expensive (\\(O(n)\\)) setting \\(v^k = w^k\\).SVRG usually used deep learning since large networks usually lie overparameterized regime. Variance reduction may useful generative adversarial networks (GAN’s).","code":""},{"path":"approximating-the-hessian.html","id":"approximating-the-hessian","chapter":"8 Approximating the Hessian","heading":"8 Approximating the Hessian","text":"Newton’s method expensive dimension \\(d\\) large requires solving \\(\\nabla^2 f(w_k) d_k = \\nabla f(w_k)\\). logistic regression costs \\(O(nd^2)\\) form Hessian \\(O(d^3)\\) solve.Many methods proposed approximate Newton’s method reduced cost -\n- Cheaper Hessian approximations\n- Hessian-free Newton method\n- Quasi-Newton methods","code":""},{"path":"approximating-the-hessian.html","id":"cheap-hessian-approximation-1---diagonal-hessian","chapter":"8 Approximating the Hessian","heading":"8.1 Cheap Hessian Approximation 1 - Diagonal Hessian","text":"Use \\(\\nabla^2 f(w^k) \\approx D^k\\) \\(D\\) diagonal matrix. makes damped newton update -\\[\nw^{k+1} = w^{k} - \\alpha_{k} D^{k} \\nabla f(w^{k})\n\\]\ncosts \\(O(d)\\).downside diagonal approximations loses superlinear convergence. problems Hessian diagonals outperforms gradient descent. many problems using Hessian diagonals worse gradient descent.","code":""},{"path":"approximating-the-hessian.html","id":"cheap-hessian-approximation-2---preconditioning","chapter":"8 Approximating the Hessian","heading":"8.2 Cheap Hessian Approximation 2 - Preconditioning","text":"Using fixed positive definite fixed matrix \\(M\\). update becomes -\\[\nw^{k+1} = w^{k} - \\alpha_{k} M \\nabla f(w^{k})\n\\]\nmatrix can chosen include second order information dot product costs \\(O(d^2)\\) less. called preconditioning.can thought performing gradient descent change variables.Using Matrix Upper Bound -\\[\n\\text{Using Lipschitz continuity assumption gradient} \\\\\n\\|\\nabla f(w) - \\nabla f(v)\\| \\leq L \\|w - v\\|. \\\\\n\\text{instead assume Lipschitz continuity respect matrix } M, \\\\\n\\|\\nabla f(w) - \\nabla f(v)\\|_M^{-1} \\leq \\|w - v\\|_M, \\\\\n\\text{} \\|d\\|_M = \\sqrt{d^T M d} \\text{ assume } M \\text{ positive definite.} \\\\\n\\text{quadratic functions, can use } M = \\nabla^2 f(w) \\text{ get Newton} \\\\\n\\text{binary logistic regression, can use } M = \\frac{1}{4} X^T X \\\\\n\\]\nmatrix-norm Lipschitz continuity leads descent lemma form\n\\[\nf(w_{k+1}) \\leq f(w_k) + \\nabla f(w_k)^T (w_{k+1} - w_k) + \\frac{1}{2} \\|w_{k+1} - w_k\\|_M^2\n\\]minimizing right side yields Newton-like step\\[\nw_{k+1} = w_k - M^{-1} \\nabla f(w_k)\n\\]\nstep require step size guarantees descent. appropriate \\(M\\) guarantees progress per iteration gradient descent. Though practice may get better performance using line-search. loose superlinear convergence cost \\(O(d^2)\\).","code":""},{"path":"approximating-the-hessian.html","id":"cheap-hessian-approximation-3---mini-batch-hessian","chapter":"8 Approximating the Hessian","heading":"8.3 Cheap Hessian Approximation 3 - Mini Batch Hessian","text":"ML problems lots data, can use mini-batch Hessian approximation\n\\[\n\\nabla^2 f(w_k) = \\frac{1}{n} \\sum_{=1}^n \\nabla^2 f_i(w_k) \\approx \\frac{1}{|B|} \\sum_{\\B} \\nabla^2 f_i(w_k), \\\\\n\\text{removes dependence } n \\text{ Hessian calculation.} \\\\\n\\]\nLeads superlinear convergence batch size grows fast enough. L2-regularized logistic regression, costs \\(O(|B|^2d + |B|^3)\\) using kernalized version.","code":""},{"path":"approximating-the-hessian.html","id":"hessian-free-newton-methods-truncated-newton","chapter":"8 Approximating the Hessian","heading":"8.4 Hessian Free Newton Methods (Truncated Newton)","text":"Cheap Hessian methods approximate \\(\\nabla^2 f(w_k)\\) lose superlinear convergence. Hessian-free Newton uses exact \\(\\nabla^2 f(w_k)\\) approximates Newton direction.strongly-convex \\(f\\) Newton’s method minimizes quadratic -\\[\n\\text{argmin}_g f(w_k) + \\nabla f(w_k)^T g + \\frac{1}{2} g^T \\nabla^2 f(w_k) g\n\\]binary logistic regression \n\\[\n\\nabla f(w) = X^T r(w), \\quad \\nabla^2 f(w) = X^T D(w) X, \\\\\n\\text{} r(w) \\text{ } D(w) \\text{ cost } O(n) \\text{ compute } n \\text{ training examples.} \\\\\n\\]Cost computing gradient \\(O(nd)\\) due matrix-vector product. Cost computing Hessian \\(O(nd^2)\\) due matrix-matrix product.\\[\n\\text{cost computing Hessian-vector product } O(nd) - \\\\\n\\nabla^2 f(w) d = X^T D(w) Xd = X^T (D(w) (Xd)), \\\\\n\\text{due matrix-vector products.}\n\\]\ndirectional derivative gradient. can compute efficiently exactly using automatic differentiation.Key ideas behind Hessian-free Newton method approximate compute Newton direction using conjugate gradient. iteration conjugate gradient needs “cheap” Hessian-vector product instead “expensive” matrix-matrix product.","code":""},{"path":"approximating-the-hessian.html","id":"quasi-newton-methods","chapter":"8 Approximating the Hessian","heading":"8.5 Quasi-Newton Methods","text":"Quasi-Newton build sequence Hessian approximations \\(B_0, B_2, B_3, ...\\) using \\(B_k\\) -\\[\nw_{k+1} = w_k - \\alpha_k B_k^{-1} \\nabla f(w_k)\n\\]\ngoal approximations eventually act like Hessian. Usually used line-search initially tries \\(\\alpha_k = 1\\).Classic quasi-Newton methods choose Bk satisfy secant equation -\\[\nB_{k+1} (w_k - w_{k-1}) = \\nabla f(w_k) - \\nabla f(w_{k-1})\n\\]iterate gradient differences Hessian information needed.","code":""},{"path":"approximating-the-hessian.html","id":"barzilai-borwein-method","chapter":"8 Approximating the Hessian","heading":"8.6 Barzilai-Borwein Method","text":"Uses approximation form \\(B_k = \\frac{1}{\\alpha_k} \\)\\(B_k\\) always solve secant equations, minimize squared error\\[\n\\alpha_{k+1} \\\\text{argmin} ||\\alpha_k B_{k+1} (w_k - w_{k-1}) - (\\nabla f(w_k) - \\nabla f(w_{k-1})) ||^2 \\\\\n\\alpha_{k+1} = \\frac{\\|w_k - w_{k-1}\\|^2}{(w_k - w_{k-1})^T (\\nabla f(w_k) - \\nabla f(w_{k-1}))}\n\\]Barzilai Borwein showed gives superlinear convergence 2d quadratics. also require inverse satisfy secant equations -\\[\n(w_k - w_{k-1}) = [B_{k+1}]^{-1} \\nabla f(w_k) - \\nabla f(w_{k-1}) \\\\\n\\text{gives alternate BB step size } \\\\\n\\alpha_{k+1} = \\frac{(w_k - w_{k-1})^T (\\nabla f(w_k) - \\nabla f(w_{k-1}))}{\\|\\nabla f(w_k) - \\nabla f(w_{k-1})\\|^2}\n\\]","code":""},{"path":"approximating-the-hessian.html","id":"bfgs-quasi-newton-method","chapter":"8 Approximating the Hessian","heading":"8.7 BFGS Quasi-Newton Method","text":"quasi-Newton methods use dense matrices \\(B_k\\). case may infinite number solutions secant equations. Many methods exist, typical methods also require \\(B_{k+1}\\) symmetric \\(B_{k+1}\\) close \\(B_k\\) normMost popular Broyden-Fletcher-Goldfarb-Shanno (BFGS) update -\\[\nB_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}, \\\\\n\\text{} s_k = w_k - w_{k-1} \\text{ } y_k = \\nabla f(w_k) - \\nabla f(w_{k-1}).\\\\\n\\]Derived rank-2 update updates stay close previous matrix norm. Cost inverting dense \\(B_k\\) \\(O(d^3)\\). can reduced \\(O(md)\\).Instead storing \\(B_k\\), store \\(m\\) vectors \\(s_k\\) \\(y_k\\). Uses update based matrix \\(H_k\\) “limited” memory. Applies BFGS update \\(m\\) times starting \\(H_k\\). Typically choose \\(H_k = \\alpha_k \\). called Limited-Memory BFGS method. default non-stochastic optimizer many cases.","code":""},{"path":"projected-gradient-based-algorithms.html","id":"projected-gradient-based-algorithms","chapter":"9 Projected Gradient Based Algorithms","heading":"9 Projected Gradient Based Algorithms","text":"","code":""},{"path":"projected-gradient-based-algorithms.html","id":"projected-gradient","chapter":"9 Projected Gradient Based Algorithms","heading":"9.1 Projected Gradient","text":"main premise optimize constraint solution can . example, might want sparse solutions model space complexity non-negative solutions 0 1 optimizing probabilities.Projected-Gradient non-negative solutions :\n\\[\nw_{k+1} = max\\{0, w_k - \\alpha_k \\nabla f(w_k)\\}\n\\]\nmax taken element wise. Sets negative values gradient descent 0.Regular projected gradient computes vanilla gradient descent iteration projects intermediate \\(w\\) value closest point satisfies constraints. Formally :projected-gradient algorithm two steps:Perform unconstrained gradient descent step:\n\\[\nw_{k+1/2} = w_k - \\alpha_k \\nabla f(w_k).\n\\]Perform unconstrained gradient descent step:\n\\[\nw_{k+1/2} = w_k - \\alpha_k \\nabla f(w_k).\n\\]Compute projection onto set \\(C\\):\n\\[\nw_{k+1} \\\\arg \\min_{v \\C} \\|v - w_{k+1/2}\\|.\n\\]\nDeriving Projected-Gradient using quadratic approximation set \\(C\\):\n\\[\nw_{k+1} \\\\arg \\min_{v \\C} \\left\\{ f(w_k) + \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2\\alpha_k} \\| v - w_k \\|^2 \\right\\} \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\alpha_k f(w_k) + \\alpha_k \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2} \\| v - w_k \\|^2 \\right\\} \\quad \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\frac{\\alpha_k^2}{2} \\| \\nabla f(w_k) \\|^2 + \\alpha_k \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2} \\| v - w_k \\|^2 \\right\\} \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\| (v - w_k) + \\alpha_k \\nabla f(w_k) \\|^2 \\right\\} \\quad (\\text{completing square}) \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\| v - (w_k - \\alpha_k \\nabla f(w_k)) \\| \\right\\} \\quad \\text{(regular gradient descent)} \\\\\nw_{k+1} = \\text{proj}_C [w_k - \\alpha_k \\nabla f(w_k)]\n\\]Compute projection onto set \\(C\\):\n\\[\nw_{k+1} \\\\arg \\min_{v \\C} \\|v - w_{k+1/2}\\|.\n\\]\nDeriving Projected-Gradient using quadratic approximation set \\(C\\):\n\\[\nw_{k+1} \\\\arg \\min_{v \\C} \\left\\{ f(w_k) + \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2\\alpha_k} \\| v - w_k \\|^2 \\right\\} \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\alpha_k f(w_k) + \\alpha_k \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2} \\| v - w_k \\|^2 \\right\\} \\quad \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\frac{\\alpha_k^2}{2} \\| \\nabla f(w_k) \\|^2 + \\alpha_k \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2} \\| v - w_k \\|^2 \\right\\} \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\| (v - w_k) + \\alpha_k \\nabla f(w_k) \\|^2 \\right\\} \\quad (\\text{completing square}) \\\\\n\\equiv \\arg \\min_{v \\C} \\left\\{ \\| v - (w_k - \\alpha_k \\nabla f(w_k)) \\| \\right\\} \\quad \\text{(regular gradient descent)} \\\\\nw_{k+1} = \\text{proj}_C [w_k - \\alpha_k \\nabla f(w_k)]\n\\]can rewrite \\(w_{k+1} = \\text{proj}_C [w_k - \\alpha_k \\nabla f(w_k)]\\) \\(w_{k+1} = \\text{proj}_C [w_k - \\alpha_k g(w^k, \\alpha_k)]\\) \\(g\\) gradient mapping defined :\\[\ng(w^k, \\alpha_k) = \\frac{1}{\\alpha_k} (w_k - \\text{proj}_C [w_k - \\alpha_k \\nabla f(w_k))\n\\]\nProjected gradient efficient cost projection cheap. costs \\(O(d)\\) adds cost iteration. Taking max 2 values just \\(O(1)\\) making projected-gradient non-negative constraints simple.","code":""},{"path":"projected-gradient-based-algorithms.html","id":"l1-regularization-to-a-constrained-problem","chapter":"9 Projected Gradient Based Algorithms","heading":"9.2 L1 Regularization to a Constrained Problem","text":"smooth objective L1 regularization:\n\\[\n\\arg \\min_{w \\\\mathbb{R}^d} \\left\\{ f(w) + \\lambda \\| w \\|_1 \\right\\}\n\\]\nCan transformed smooth problem non-negative constraints:\n\\[\n\\arg \\min_{w^+ \\geq 0, w^- \\geq 0} \\left\\{ f(w^+ - w^-) + \\lambda \\sum_{j=1}^d (w^+_j + w^-_j) \\right\\}\n\\]Essentially splitting \\(w\\) difference 2 non-negative vectors. Turning non-smooth objective, smooth. can now apply projected gradient transformed yet equivalent objective.","code":""},{"path":"projected-gradient-based-algorithms.html","id":"active-set-identification-and-backtracking","chapter":"9 Projected Gradient Based Algorithms","heading":"9.3 Active Set Identification and Backtracking","text":"L1 regularization identifies “active-set” projected gradient. active set selects features leaving weights seem relevant non-zero.sufficiently large \\(k\\), sparsity pattern \\(w_k\\) matches sparsity pattern \\(w^*\\).\n\\[\nw^0 =\n\\begin{pmatrix}\nw^0_1 \\\\\nw^0_2 \\\\\nw^0_3 \\\\\nw^0_4 \\\\\nw^0_5\n\\end{pmatrix}\n\\text{finite } k \\text{ iterations} \\quad\nw^k =\n\\begin{pmatrix}\nw^k_1 \\\\\n0 \\\\\n0 \\\\\nw^k_4 \\\\\n0\n\\end{pmatrix},\n\\quad \\text{} \\quad\nw^* =\n\\begin{pmatrix}\nw^*_1 \\\\\n0 \\\\\n0 \\\\\nw^*_4 \\\\\n0\n\\end{pmatrix}\n\\]can also use 2 step sizes:Consider introducing second step size \\(\\eta_k \\leq 1\\),\n\\[\nw_{k+1} = w_k - \\eta_k \\alpha_k g(w_k, \\alpha_k)\n\\]\naffects far move gradient mapping direction.2 Backtracking Strategies:Backtracking along feasible direction. Fix \\(\\alpha_k\\) backtrack reducing \\(\\eta_k\\). 1 projection per iteration (good projection expensive). guaranteed identify active set.Backtracking along feasible direction. Fix \\(\\alpha_k\\) backtrack reducing \\(\\eta_k\\). 1 projection per iteration (good projection expensive). guaranteed identify active set.Backtracking along projection. Fix \\(\\eta_k\\) 1 backtrack reducing \\(\\alpha_k\\). 1 projection per backtracking step (bad projection expensive). identifies active set finite number iterations.Backtracking along projection. Fix \\(\\eta_k\\) 1 backtrack reducing \\(\\alpha_k\\). 1 projection per backtracking step (bad projection expensive). identifies active set finite number iterations.","code":""},{"path":"projected-gradient-based-algorithms.html","id":"accelerating-projection-methods","chapter":"9 Projected Gradient Based Algorithms","heading":"9.4 Accelerating Projection Methods","text":"accelerated projected-gradient method form:\n\\[\nw_{k+1} = \\text{proj}_C [v_k - \\alpha_k \\nabla f(w_k)] \\\\\nv_{k+1} = w_{k+1} + \\beta_k (w_{k+1} - w_k).\n\\]achieves accelerated rate unconstrained case. \\(v_k\\) might satisfy constraints.Using Newton’s method:naive Newton-like methods Hessian \\(H_k\\):\n\\[\nw_{k+1} = \\text{proj}_C \\left[ w_k - \\alpha_k (H_k)^{-1} \\nabla f(w_k) \\right].\n\\]\nwork can point wrong directions.correct projected-Newton method uses\n\\[\nw_{k+1/2} = w_k - \\alpha_k (H_k)^{-1} \\nabla f(w_k) \\\\\nw_{k+1} = \\arg \\min_{v \\C} \\| v - w_{k+1/2} \\|_{H_k} \\quad (\\text{projection Hessian metric})\n\\]Projected-gradient minimizes quadratic approximation:\n\\[\nw_{k+1} = \\arg \\min_{v \\C} \\left\\{ f(w_k) + \\nabla f(w_k)(v - w_k) + \\frac{1}{2\\alpha_k} \\| v - w_k \\|^2 \\right\\}\\\\\nw_{k+1} = \\arg \\min_{v \\\\mathbb{R}^d} \\left\\{ f(w_k) + \\nabla f(w_k)(v - w_k) + \\frac{1}{2\\alpha_k} (v - w_k)H_k(v - w_k) \\right\\}\\\\\nw_{k+1} = \\arg \\min_{v \\C} \\left\\{ f(w_k) + \\nabla f(w_k)(v - w_k) + \\frac{1}{2\\alpha_k} (v - w_k)H_k(v - w_k) \\right\\}\n\\]\nEquivalently, project Newton step Hessian-defined norm:\n\\[\nw_{k+1} = \\arg \\min_{v \\C} \\left\\| v - \\left(w_k - \\alpha_t H_k^{-1} \\nabla f(w_k)\\right) \\right\\|_{H_k}\n\\]\nexpensive can dealt approximating step. \\(H_k\\) diagonal, simple. can also make matrix “diagonal” using two metric projection ().Two-Metric ProjectionConsider optimizing non-negative constraints, \\[ \\min_{w \\C} f(w) \\].\ntwo-metric projection method splits variables two sets:\n\\[\nA_k \\equiv \\{ \\mid w_k^= 0, \\nabla_i f(w_k) > 0 \\}\\\\\nI_k \\equiv \\{ \\mid w_k^\\neq 0 \\text{ } \\nabla_i f(w_k) \\leq 0 \\},\n\\]\nactive variables (constrained) “inactive variables”. use projected-gradient step \\(A_k\\) naive projected-Newton \\(I_k\\).\n\\[\nw_{k+1}^{A_k} = \\text{proj}_C \\left[ w_k^{A_k} - \\alpha_k \\nabla_{A_k} f(w_k) \\right] \\\\\nw_{k+1}^{I_k} = \\text{proj}_C \\left[ w_k^{I_k} - \\alpha_k \\left( \\nabla_{I_k}^2 f(w_k) \\right)^{-1} \\nabla_{I_k} f(w_k) \\right]\n\\]\nEventually switches unconstrained Newton unconstrained variables.","code":""},{"path":"projected-gradient-based-algorithms.html","id":"projected-sgd-and-cd","chapter":"9 Projected Gradient Based Algorithms","heading":"9.5 Projected SGD and CD","text":"Projected Stochastic Gradient Descent:\n\\[\nw_{k+1} = \\text{proj}_C \\left[ w_k - \\alpha_k \\nabla f_{i_k}(w_k) \\right]\n\\]\nprojected gradient random training example \\(i_k\\).properties SGD projected-gradient hold:\n- Lose fast convergence -parameterized models longer even \\(\\nabla f(w^*) = 0\\)\n- Lose active set identification property projected gradient.Variant restores property dual averaging:\n\\[\nw_{k+1} = \\text{proj}_C \\left[ w_0 - \\alpha_k \\sum_{t=1}^k \\nabla f(w_k) \\right],\n\\]Since uses average previous gradients variance direction goes 0.","code":""},{"path":"projected-gradient-based-algorithms.html","id":"frank-wolfe-method","chapter":"9 Projected Gradient Based Algorithms","heading":"9.6 Frank-Wolfe Method","text":"Frank Wolfe method uses linear approximation function instead quadratic. quadratic approximation harder compute sometimes simpler linear approximation can trick.\\[\nargmin_{ v \\C} \\left\\{ f(w_k) + \\nabla f(w_k)^\\top (v - w_k) \\right\\}\n\\]\nset \\(C\\) must bounded otherwise solution may exist. can move \\(v\\) won’t solution finite steps.algorithm :\\[\nw_{k+1} = w_k + \\alpha_k (v_k - w_k)\\\\\nv_k \\argmin_{v \\C} \\nabla f(w_k)^\\top v\\\\\n\\]\ngradient mapping :\n\\[\n\\frac{1}{\\alpha_k} (w_k - v_k).\n\\]\nCan used line search. convergence rate \\(O(\\frac{1}{k})\\).","code":""},{"path":"global-optimization-and-subgradients.html","id":"global-optimization-and-subgradients","chapter":"10 Global Optimization and Subgradients","heading":"10 Global Optimization and Subgradients","text":"Real valued functions extremely tricky optimize globally.Consider minimizing function unit hyper-cube:\n\\[\n\\min_{w \\[0,1]^d} f(w),\n\\]Using algorithm, can construct \\(f(w_k) - f(w^*) > \\epsilon\\) forever due infinite real numbers two real numbers.","code":""},{"path":"global-optimization-and-subgradients.html","id":"considering-minimizing-lipschitz-continuous-functions","chapter":"10 Global Optimization and Subgradients","heading":"10.1 Considering Minimizing Lipschitz-Continuous Functions","text":"\\[\n\\vert f(w) - f(v) \\vert \\leq L \\lVert w - v \\rVert\n\\]Functions don’t change arbitrarily fast change \\(x\\).Considering unit hypercube , becomes easier optimize. worst case \\(O(\\frac{1}{\\epsilon^d})\\) achieved simple grid search.can go faster use random guesses. Lipschitz-continuity implies ball \\(\\epsilon\\) optimal solutions around \\(w^*\\). radius ball \\(\\Omega(\\epsilon)\\)“volume” \\(\\Omega(\\epsilon^d)\\).","code":""},{"path":"global-optimization-and-subgradients.html","id":"subgradient-methods","chapter":"10 Global Optimization and Subgradients","heading":"10.2 Subgradient Methods","text":"subgradient?Differentiable convex functions always tangent lines:\\[\nf(v) \\geq f(w) + \\nabla f(w)^\\top (v - w), \\quad \\forall w, v.\n\\\n\\]\n\\[\n\\text{vector } d \\text{ subgradient convex function } f \\text{ } w \\text{ }\\\\\nf(v) \\geq f(w) + d^\\top (v - w), \\quad \\forall v.\n\\]\nexample, sub-differential absolute value function:\n\\[\n\\partial |w| =\n\\begin{cases}\n1 & \\text{} w > 0 \\\\\n-1 & \\text{} w < 0 \\\\\n[-1, 1] & \\text{} w = 0\n\\end{cases}\n\\]L1 regularization give sparsity?Considering L2-regularized least squares:\n\\[\nf(w) = \\frac{1}{2} \\| Xw - y \\|_2^2 + \\frac{\\lambda}{2} \\| w \\|_2^2.\n\\]\nElement \\(j\\) gradient \\(w_j = 0\\) given \n\\[\n\\nabla_j f(w) = x_j^\\top (Xw - y) \\big| + \\lambda w\n\\]\n\\(w_j = 0\\) solution need \\(\\nabla_j f(w^*) = 0\\) \\(0 = x_j^\\top r^*\\)\n\\(r^* = Xw^* - y\\) solution \\(w^*\\) column \\(j\\)orthogonal final residual.possible, unlikely. Increasing \\(\\lambda\\) doesn’t help.Considering L1-regularized least squares:\n\\[\nf(w) = \\frac{1}{2} \\| Xw - y \\|_2^2 + \\lambda \\| w \\|_1.\n\\]\nElement \\(j\\) subdifferential \\(w_j = 0\\) given \n\\[\n\\partial_j f(w) \\equiv x_j^\\top (Xw - y) \\big| + \\lambda [-1, 1] .\n\\]\n\\(w_j = 0\\) solution, need \\(0 \\\\partial_j f(w^*)\\) \n\\[\n0 \\x_j^\\top r^* + \\lambda [-1, 1] \\\\\n-x_j^\\top r^* \\\\lambda [-1, 1] \\\\\n|x_j^\\top r^*| \\leq \\lambda,\n\\]\nfeatures \\(j\\) little \\(y\\) often lead \\(w_j = 0\\). Increasing \\(\\lambda\\) makes likely happen.subgradient method:\\[\nw_{k+1} = w_k - \\alpha_k g_k,\n\\]\n\\(g_k \\\\partial f(w_k)\\).non-differentiable point, subgradients may reduce objective. differentiable points, subgradient gradient reduces objective.subgradient method applied Lipschitz convex \\(f\\) \n\\[\n\\|w_{k+1} - w^*\\|^2 = \\|w_k - w^*\\|^2 - 2\\alpha_k g_k^\\top (w_k - w^*) + \\alpha_k^2 ||g_k||^2\\\\\n\\leq \\|w_k - w^*\\|^2 - 2\\alpha_k[f(w_k) - f(w^*)] + \\alpha_k^2 L^2.\\\\\\\\\n\\rightarrow 2\\alpha_k[f(w_k) - f(w^*)] \\leq \\|w_k - w^*\\|^2 - \\|w_{k+1} - w^*\\|^2 + \\alpha_k^2 L^2\n\\]\nsumming telescoping values \\(k = 1\\) \\(t\\) get\n\\[\n2 \\sum_{k=1}^t \\alpha_k[f(w_k) - f(w^*)] \\leq \\|w_0 - w^*\\|^2 - \\|w_{k+1} - w^*\\|^2 + L^2 \\sum_{k=1}^t \\alpha_k.\n\\]Using \\(f_b\\) lowest \\(f(w_k)\\) \\(\\|w_{k+1} - w^*\\|^2 \\geq 0\\) can re-arrange get bound similar showed SGD:\n\\[\nf(w_b) - f(w^*) \\leq \\frac{\\|w_1 - w^*\\|^2 + L^2 \\sum_{k=1}^t \\alpha_k^2}{2 \\sum_{k=1}^t \\alpha_k}\n= O \\left( \\frac{1}{\\sum_{k} \\alpha_k} \\right) + O \\left( \\frac{\\sum_{k} \\alpha_k^2}{\\sum_{k} \\alpha_k} \\right)\n\\]\n## Smooth ApproximationsNon smooth functions can approximated using smooth functions. Non smooth regions can smoothed leaving rest function untouched. Like Huber loss absolute value function.advantage smoothing can faster conversions sub gradient methods. can use line search momentum acceleration faster convergence. Huber smoothed objective functions often similar test error.reasons smooth smoothing can destroy structure solution. example, L1 regularization leads sparse solutions smooth Huber loss lead sparse solutions. Smooth approximations can expensive evaluate don’t converge faster add stochasticity.","code":""},{"path":"global-optimization-and-subgradients.html","id":"linear-convergence","chapter":"10 Global Optimization and Subgradients","heading":"10.3 Linear convergence","text":"Bisection: Linear convergence 1 dimension -Consider following method finding minimizer:iteration, compute subgradient middle interval.Set lower/upper bound interval midpoint (using subgradient sign)Maximum distance \\(w^*\\) cut half giving iteration complexity \\(O(log(1/\\epsilon))\\)Cutting Plane: Linear Convergence \\(d\\) dimensions -generalizes bijections higher dimensions. Can used optimize convex functions bounded polygons.iteration, compute subgradient center polygon. definition subgradient \\(w\\) \\(f(w) \\geq f(w_k) +g_k^\\top (w - w_k)\\). \\(w\\) satisfying \\(g_k^\\top (w - w_k) > 0\\) greater \\(f(w_k)\\).iteration, compute subgradient center polygon. definition subgradient \\(w\\) \\(f(w) \\geq f(w_k) +g_k^\\top (w - w_k)\\). \\(w\\) satisfying \\(g_k^\\top (w - w_k) > 0\\) greater \\(f(w_k)\\).constraint analogous plane cuts polygon.constraint analogous plane cuts polygon.Worst-case theoretical rates convex optimization subgradients :Best subgradient methods require \\(O(1/\\epsilon^2)\\) iterations.\nBest cutting plane methods require \\(O(d log(1/\\epsilon))\\) iterations","code":""},{"path":"global-optimization-and-subgradients.html","id":"using-multiple-subgradients","chapter":"10 Global Optimization and Subgradients","heading":"10.4 Using Multiple Subgradients","text":"get tighter bound using previous function subgradient values:\n\\[\nf(w) \\geq \\max_{t \\\\{1, \\ldots, k\\}} f(w_t) + g_t^\\top (w - w_t)\n\\]\ncan also choose “best” subgradient?\nConvex functions directional derivatives everywhere. Direction \\(-g_t\\) minimizes directional derivative minimum-norm subgradient:\n\\[\ng_k \\\\arg\\min_{g \\\\partial f(w_k)} \\|g\\|\n\\]\nsteepest descent direction non-smooth convex optimization problems.advantages solution fixed point: \\(w^* = w^* - \\alpha g^*\\) since \\(g^* = 0\\). can satisfy line-search criteria since \\(-g_k\\) descent direction.\n- line searches work directional derivatives, exist.issues minimum-norm subgradient may difficult find. Convergence well understood shown improve worst-case rate subgradient method. Counter-examples exist line search causes convergence sub-optimal values.Optimizing smooth \\(f\\) (non-smooth) L1-regularization,\n\\[\n\\text{argmin}_w f(w) + \\lambda \\|w\\|_1.\n\\]\nsubdifferential respect coordinate \\(j\\) form:\n\\[\n\\nabla_j f(w) + \\lambda \\begin{cases}\n\\text{sign}(w_j) & w_j \\neq 0 \\\\\n[-1, 1] & w_j = 0\n\\end{cases}.\n\\]\npart subdifferential smallest absolute value given \n\\[\n\\begin{cases}\n\\nabla_j f(w) + \\lambda \\text{sign}(w_j) & \\text{} w_j \\neq 0 \\\\\n\\nabla_j f(w) - \\lambda \\text{sign}(\\nabla_j f(w)) & \\text{} w_j = 0, |\\nabla_j f(w)| > \\lambda \\\\\n0 & \\text{} w_j = 0, |\\nabla_j f(w)| \\leq \\lambda\n\\end{cases}.\n\\]\ncan viewed steepest descent direction L1-regularization. keeps variables 0 partial derivative zero small enough. However, min-norm subgradient automatically set variables 0.Orthant-Projected Min-Norm Subgradient L1-regularizationMin-norm subgradient method orthant projection L1-regularization :\n\\[\nw_{k+1} = \\text{proj}_O(w_k) [w_k - \\alpha_k g_k]\\\\\n\\text{, } g_k \\\\arg\\min_{g \\\\partial f(w_k)} \\|g\\|\n\\]\n\\(\\text{proj}_O(w_k) [z]\\) sets \\(z_j = 0\\) $(z_j) (w_j) $.\\(w_{k+1}\\) stays orthant \\(w_k\\).lot appealing properties:\n- Orthant-project can result sparse solutions.\n- Min-norm subgradient keeps values 0.\n- Can combined line-search.\n- Can use clever step sizes like Barzilai Borwein.","code":""},{"path":"proximal-gradient-methods.html","id":"proximal-gradient-methods","chapter":"11 Proximal Gradient Methods","heading":"11 Proximal Gradient Methods","text":"Proximal-gradient methods apply functions form:\\[\nF(w) = f(w) + r(w)\\\\\n\\text{} f \\text{ simple } r \\text{ simple can non-smooth.}\n\\]function minimize:\n\\[\n\\arg\\min_{w \\\\mathbb{R}^d} f(w) + r(w).\n\\]\nIteration \\(w_k\\) works quadratic approximation \\(f\\):\n\\[\nf(v) + r(v) \\approx f(w_k) + \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2\\alpha_k} \\|v - w_k\\|^2 + r(v),\n\\]\n\\[\nw_{k+1} \\\\arg\\min_{v \\\\mathbb{R}^d} \\left[ f(w_k) + \\nabla f(w_k)^\\top (v - w_k) + \\frac{1}{2\\alpha_k} \\|v - w_k\\|^2 + r(v) \\right]\n\\]\nWriting proximal optimization:\n\\[\nw_{k+1} \\\\arg\\min_{v \\\\mathbb{R}^d} \\left[ \\frac{1}{2} \\|v - (w_k - \\alpha_k \\nabla f(w_k))\\|^2 + \\alpha_k r(v) \\right]\\\\\nw_{k+1} = \\text{prox}_{\\alpha_k r} [w_k - \\alpha_k \\nabla f(w_k)]\n\\]\nproximal-gradient algorithm:\n\\[\nw_{k+\\frac{1}{2}} = w_k - \\alpha_k \\nabla f(w_k), \\quad w_{k+1} = \\arg\\min_{v \\\\mathbb{R}^d} \\left[ \\frac{1}{2} \\|v - w_{k+\\frac{1}{2}}\\|^2 + \\alpha_k r(v) \\right]\n\\]\nright equation called proximal operator respect convex function \\(\\alpha_k r\\). can efficiently compute proximal operator, \\(r\\) “simple”.L1-regularization can optimized using Proximal-Gradient:\n\\[\n\\text{prox}_{\\alpha_k \\lambda \\|\\cdot\\|_1} [w] \\\\arg\\min_{v \\\\mathbb{R}^d} \\left[ \\frac{1}{2} \\|v - w\\|^2 + \\alpha_k \\lambda \\|v\\|_1 \\right]\\\\\nw_j \\\\arg\\min_{v_j \\\\mathbb{R}} \\left[ \\frac{1}{2} (v_j - w_j)^2 + \\alpha_k \\lambda |v_j| \\right]\n\\]","code":""},{"path":"proximal-gradient-methods.html","id":"convergence-rate-1","chapter":"11 Proximal Gradient Methods","heading":"11.1 Convergence Rate","text":"Proximal-Gradient Linear Convergence RateSimplest linear convergence proofs based proximal-PL inequality,\n\\[\n\\frac{1}{2} D_r(w, L) \\geq \\mu(F(w) - F^*)\\\\\n\\text{} \\|\\nabla f(w)\\|^2 \\text{ PL inequality generalized }\\\\\nD_r(w, L) = -2\\alpha \\min_v \\left[ \\nabla f(w)^\\top (v - w) + \\frac{L}{2} \\|v - w\\|^2 + r(v) - r(w) \\right]\n\\]\nLinear convergence \\(\\nabla f\\) Lipschitz \\(F\\) proximal-PL:\n\\[\nF(w_{k+1}) = f(w_{k+1}) + r(w_{k+1})\\\\\n\\leq f(w_k) + \\langle \\nabla f(w_k), w_{k+1} - w_k \\rangle + \\frac{L}{2} \\|w_{k+1} - w_k\\|^2 + r(w_{k+1})\\\\\n= F(w_k) + \\langle \\nabla f(w_k), w_{k+1} - w_k \\rangle + \\frac{L}{2} \\|w_{k+1} - w_k\\|^2 + r(w_{k+1}) - r(w_k)\\\\\n\\leq F(w_k) - \\frac{\\mu}{L} [F(w_k) - F^*] \\quad \\text{proximal-PL}\n\\]","code":""},{"path":"proximal-gradient-methods.html","id":"dualty","chapter":"11 Proximal Gradient Methods","heading":"11.2 Dualty","text":"Considering support vector machine optimization problem:\n\\[\n\\arg\\min_{w \\\\mathbb{R}^d} C \\sum_{=1}^{n} \\max\\{0, 1 - y_i w^\\top x_i\\} + \\frac{1}{2} \\|w\\|^2\n\\]\n\\(C\\) regularization parameter (\\(\\lambda = 1/C\\)).problem non-smooth strongly-convex, proximal operator simple. use stochastic subgradient, converges slowly. hard set step size.Fenchel dual SVM optimization given :\\[\n\\arg\\max_{z \\\\mathbb{R}^n \\,|\\, 0 \\leq z \\leq C} \\sum_{=1}^{n} z_i - \\frac{1}{2} \\|X^\\top Y z\\|^2,\n\\]\n\\(X\\) vectors \\(x_i^T\\) rows \\(Y\\) diagonal \\(y_i\\) along diagonal.\\(d \\times n\\) matrix \\(= X^\\top Y\\), SVM dual problem can written :\\[\n\\arg\\max_{0 \\leq z \\leq C} z^\\top 1 - \\frac{1}{2} \\|Az\\|^2\n\\]\ndual transforms problem space optimization easier. can use faster method optimal step size.dual solution \\(z*\\), primal solution \\(w* = Az*\\). Solving dual problem allows us solve primal problem efficiently. dual Lipschitz-smooth, L maximum eigenvalue \\(^TA\\). Additionally, since constraints simple, can apply projected gradient methods. dual problem satisfies proximal-PL condition, µ minimum non-zero singular value \\(^TA\\), enabling linear convergence rates. Moreover, constraints separable, dual amenable random coordinate optimization, projected randomized coordinate optimization achieves linear convergence rate. Optimal step sizes can derived straightforwardly, eliminating need backtracking.","code":""},{"path":"proximal-gradient-methods.html","id":"supremum-and-infimum","chapter":"11 Proximal Gradient Methods","heading":"11.3 Supremum and Infimum","text":"Infimum generalization min also includes limits:\n\\[\n\\min_{x \\\\mathbb{R}} x^2 = 0, \\quad \\inf_{x \\\\mathbb{R}} x^2 = 0\\\\\n\\min_{x \\\\mathbb{R}} e^x = \\text{DNE}, \\quad \\inf_{x \\\\mathbb{R}} e^x = 0\n\\]\nFormally, infimum function \\(f\\) largest lower bound:\n\\[\n\\inf f(x) = \\max_{y} \\left\\{ y \\,|\\, y \\leq f(x) \\right\\}.\n\\]\nSupremum smallest upper bound.Convex ConjugateThe convex conjugate \\(f^*\\) function \\(f\\) given \n\\[\nf^*(y) = \\sup_{x \\X} \\{ y^Tx - f(x) \\},\n\\]\n\\(X\\) set values supremum finite. ’s maximum linear function \\(f(x)\\).ML primal problem usually:\n\\[\n\\arg\\min_{w \\\\mathbb{R}^d} P(w) = f(Xw) + r(w).\n\\]\nintroduce equality constraints,\n\\[\n\\arg\\min_{v=Xw} f(v) + r(w).\n\\]\nLagrangian dual special form called Fenchel dual.\n\\[\n\\arg\\max_{z \\\\mathbb{R}^n} D(z) = -f^*(-z) - r^*(X^\\top z)\n\\]\nPrimal dual functions nutshell:\\[\nP(w) = f(Xw) + r(w) \\\\\nD(z) = -f^*(-z) - r^*(X^\\top z)\n\\]\nNumber dual variables \\(n\\) instead \\(d\\).\n- Dual may lower-dimensional problem.\n- Weak duality \\(P(w) \\geq D(z)\\) \\(w\\) \\(z\\) (assuming \\(P\\) bounded ). value dual objective gives lower bound \\(P(w^*)\\).\n- Lipschitz-smoothness strong-convexity relationship.\n- Dual Lipschitz-smooth primal strongly convex (SVMs).\n- Dual loss \\(f^*\\) separable \\(f\\) finite-sum problem. allows us use dual coordinate optimization many problems.\n- Strong duality holds \\(P(w^*) = D(z^*)\\). requires additional assumption.\nExample: \\(f\\) \\(g\\) convex, exists feasible \\(w\\) \\(z = Xw\\) \\(g\\) continuous \\(z\\). true, can use duality gap \\(P(w) - D(z)\\) certify optimality \\(w\\) \\(z\\).","code":""},{"path":"optimization-demo.html","id":"optimization-demo","chapter":"12 Optimization Demo","heading":"12 Optimization Demo","text":"Various optimization algorithms shown Rosenbrock function.Gradient descent baseline 1000 generations minimized objective function, reach minima.Backtracking able go vanilla gradient descent fewer iterations adjusting learning rate progress condition met.","code":""},{"path":"optimization-demo.html","id":"gradient-descent-with-momentum","chapter":"12 Optimization Demo","heading":"12.1 Gradient Descent with Momentum","text":"Momentum shown use iterations compared gradient descent without backtracking, tendency overshoot minima considers previous parameter values accelerate optimization.","code":""},{"path":"optimization-demo.html","id":"gradient-descent-with-nesterov-acceleration","chapter":"12 Optimization Demo","heading":"12.2 Gradient Descent with Nesterov Acceleration","text":"Nesterov Acceleration natural tend shoot minimum taking consideration previous parameters providing accelerated convergence rate.","code":""},{"path":"optimization-demo.html","id":"nesterov-with-restart","chapter":"12 Optimization Demo","heading":"12.3 Nesterov With Restart","text":"Restarts used Nesterov acceleration “course correct” optimization path. gives similar results gradient descent backtracking fewer iterations.","code":""},{"path":"optimization-demo.html","id":"newtons-method","chapter":"12 Optimization Demo","heading":"12.4 Newton’s Method","text":"Newton’s method chooses step size using Hessian objective function. iteration efficient function reaches minimum just six iterations However, Hessian expensive calculate. part bit erratic deal damp Newton update.","code":""},{"path":"optimization-demo.html","id":"damped-newtons-method","chapter":"12 Optimization Demo","heading":"12.5 Damped Newton’s Method","text":"Damping Newton update even 0.999 \\((0.999 \\times \\text{original update})\\) helps lot making optimization stable. took iterations converge.","code":""},{"path":"optimization-demo.html","id":"coordinate-descent","chapter":"12 Optimization Demo","heading":"12.6 Coordinate Descent","text":"Coordinate descent optimizes one variable time, can clearly seen nature optimization path. case, coordinate descent, took quite iterations reach minima.","code":""},{"path":"connvolutional-neural-networks.html","id":"connvolutional-neural-networks","chapter":"13 Connvolutional Neural Networks","heading":"13 Connvolutional Neural Networks","text":"","code":""},{"path":"connvolutional-neural-networks.html","id":"basics","chapter":"13 Connvolutional Neural Networks","heading":"13.1 Basics","text":"Basic neural networks take perform linear transformation followed non-linear function.\\[\nz = h(Wx + b)\\\\\n\\text{single hidden layer network } W \\text{ learnable matrix } b \\text{ learnable vector.}\n\\]layers can stacked together larger network consists “dense” fully connected linear layers pass non-linear “activation” function moving onto next layer.\\[\ny = h_3(W_3 \\cdot (h_2(W_2 \\cdot h_1(W_1 x + b+1) + b_2)) + b)\\\\\n\\text{network 3 hidden layers.}\n\\]data comes mapped space linearly separable. mapping learned data using backpropagation variations gradient descent.Need convolutional neural networks?issue dense networks number parameters sizes inputs quickly gets hand. tiny \\(100 \\times 100\\) gray image \\(10,000\\) parameters. Assuming output predicting number picture digit , size output layer 10 (0 - 9 digits). yields network least \\(10,000 \\times 10 = 100, 000\\) parameters. huge impractically small image size modern networks output sizes tens hundreds thousands.second issue feed picture dense neural network, flatten image’s \\(n\\)-dimensional array. \\(100 \\times 100\\) image matrix becomes \\(10,000\\) sized vector. leads total loss structure image means completely ignore spatial locality helps us identify picture contains. example, picture apple likely specific curvature.need way reduce size input leading reduced set parameters. need way learn spacial localities data. convolutional neural networks come play.","code":""},{"path":"connvolutional-neural-networks.html","id":"convolutions","chapter":"13 Connvolutional Neural Networks","heading":"13.2 Convolutions","text":"Assume discrete data height width, like gray image. convolution can mathematically expressed :\\[\n(* K)(, j) = \\sum_{m=-k}^{k} \\sum_{n=-k}^{k} (-m, j-n) \\cdot K(m, n)\\\\\n\\text{} \\text{ image.}\\\\\n\\text{} K \\text{ relatively tiny matrix called kernel filter.}\n\\]Intuitively, operation slides tiny matrix image computes dot product stage. changes image highlights things tat resemble kernel. can pass convolved image non linear function \\(h\\) produce complex patterns.Convolutions help capture spacial localities data. especially useful images learn underlying patterns. , convolutions helpful kinds data gain meaning “around” features data. example, speech data, flow fields signals. notion useful neural networks can found convolutional layers.","code":""},{"path":"connvolutional-neural-networks.html","id":"convolutional-layers","chapter":"13 Connvolutional Neural Networks","heading":"13.3 Convolutional Layers","text":"Convolutional layers allow neural networks learn spatial localities data example flow fluid features image. Convolutional filters randomly initialized training learn highlight certain teaches data. can drastically reduce number parameters needed network.Filters typically depth impact receive filter convolved image. bias added, whole thing passed non-linearity assuming use D filters get output depth D particular layer.picture, large image depth 4 convolved two small filters also depth 4. bias added output fed non-linear function. output depth 2.size image convolution course depends stride (many pixels skip slide ) padding (deal boundary).","code":""},{"path":"connvolutional-neural-networks.html","id":"pooling-layers","chapter":"13 Connvolutional Neural Networks","heading":"13.4 Pooling layers","text":"Pooling layers include learnable parameters. used reduce height width data well build invariances data help generalization making model robust. two types pooling - max pooling average pooling.Pooling done using \\(k \\times k\\) filter slid image either maximum average value numbers kernel kept thereby reducing size data passing pooling layer. layers typically placed convolutional layers.","code":""},{"path":"connvolutional-neural-networks.html","id":"example","chapter":"13 Connvolutional Neural Networks","heading":"13.5 Example","text":"example convolutional neural network two convolutional layers two linear layers:\\[\nI_1(c_1, , j) = h_{conv_1}\\left[\\sum_{c=1}^{3} \\sum_{m=-k_1}^{k_1} \\sum_{n=-k_1}^{k_1} (c, -m, j-n) \\cdot K_1(c_1, c, m, n) + b_{conv_1}\\right] \\\\\\\\\n\\quad \\\\\nP_1(c_1, , j) = \\max_{m,n \\\\{0, \\ldots, p_1-1\\}} I_1(c_1, \\cdot p_1 + m, j \\cdot p_1 + n) \\\\\\\\\n\\quad \\\\\nI_2(c_2, , j) = h_{conv_2}\\left[\\sum_{c_1=1}^{C_1} \\sum_{m=-k_2}^{k_2} \\sum_{n=-k_2}^{k_2} P_1(c_1, -m, j-n) \\cdot K_2(c_2, c_1, m, n)+ b_{conv_2}\\right] \\\\\\\\\n\\quad \\\\\nP_2(c_2, , j) = \\max_{m,n \\\\{0, \\ldots, p_2-1\\}} I_2(c_2, \\cdot p_2 + m, j \\cdot p_2 + n) \\\\\\\\\n\\quad \\\\\nf = \\text{flatten}(P_2) \\\\\\\\\n\\quad \\\\\nz_1 = h_1(W_1 f + b_1) \\\\\\\\\n\\quad \\\\\nY = h_2(W_2 z_1 + b_2) \\\\\\\\\n\\]","code":""},{"path":"connvolutional-neural-networks.html","id":"conclusion","chapter":"13 Connvolutional Neural Networks","heading":"13.6 Conclusion","text":"Convolutional layers allow us extract features input hierarchical way. Let us see training CNN predict image human, dog cat using say 3 convolutional layers followed fully connected layers output three neurons.first convolutional layer might certain set filters detect edges boundaries. second convolutional might set filters detect eyes, noses, ears hair. third set filters convolutional layers might detect limbs appendages. stage filters might light see, example, see human hand light see dogs paw. condensed information flattened passed fully connected layers finally predict image .information see paper.Fully Convolutional U-NET Type architectureThis prediction (test set) using simplified UNET based fully convolutional neural network.network takes data distance functions object flow region channel number represents information flow - 0 obstacle, 1 free flow region, 2 upper/bottom -slip wall condition, 3 constant velocity inlet condition, 4 zero-gradient velocity outlet condition.","code":""},{"path":"recurrent-neural-networks.html","id":"recurrent-neural-networks","chapter":"14 Recurrent Neural Networks","heading":"14 Recurrent Neural Networks","text":"","code":""},{"path":"recurrent-neural-networks.html","id":"background","chapter":"14 Recurrent Neural Networks","heading":"14.1 Background","text":"problem fixed size neural networks, even convolutional layers, thy handle input variable input size like sentence \\(k\\) words. pass one word time input treated independently. images, pass 10 frames video convolutional networks one wont relation temporal component video. frame treated separate image. regular neural networks don’t take account past inputs sequence.\\[\ny = h(Wx+b)\n\\]","code":""},{"path":"recurrent-neural-networks.html","id":"introduction-1","chapter":"14 Recurrent Neural Networks","heading":"14.2 Introduction","text":"handle sequential data, can define neural network updates ’s weights based previous data sequence.\\[\nh_t = f_1(W_{h} h_{t-1} + W_{x} x_{t} + b_h) \\\\\ny = f_2(W_{y} h_{t} + b_y)\n\\], \\(W_{x} x_{t}\\) standard matrix multiply used regular neural networks. \\(W_hh_{t-1}\\) relates inputs sequence together. \\(W_{h}, W_{x}, W_{h}, b_h, b_y\\) shared across temporal time component \\(t\\). time-step \\(t\\), new \\(h\\) \\(y\\) calculated. \\(x_0\\) usually initialized \\(0\\) vector randomly. intuition weights updated taking account whole sequence.","code":""},{"path":"recurrent-neural-networks.html","id":"types-of-rnns","chapter":"14 Recurrent Neural Networks","heading":"14.3 Types of RNN’s","text":"RNN’s can represented -cyclic arrow represents network updating time interval \\(t\\). can used represent recurrent neural network can concrete. common arrangements can useful different situations.","code":""},{"path":"recurrent-neural-networks.html","id":"one-to-many","chapter":"14 Recurrent Neural Networks","heading":"14.3.1 One to many","text":"networks take one input (sequence size 1), produce sequence size. example, input may image fixed size output caption sequence size.","code":""},{"path":"recurrent-neural-networks.html","id":"many-to-one","chapter":"14 Recurrent Neural Networks","heading":"14.3.2 Many to one","text":"networks take input sequence variable size give us one output. example, input may sentence sequence variable size output sentiment sentence.","code":""},{"path":"recurrent-neural-networks.html","id":"many-to-many","chapter":"14 Recurrent Neural Networks","heading":"14.3.3 Many to Many","text":", input output variable sequence length. example, using deep learning translate one language another. also video multiple frame can segmentation classification segmentation frames image.","code":""},{"path":"recurrent-neural-networks.html","id":"problems-with-basic-rnns","chapter":"14 Recurrent Neural Networks","heading":"14.4 Problems with (Basic) RNN’s","text":"backpropagation “unrolled” RNN. run problems gradient passes unrolled network. concretely:Let us consider 3 iterations \\(h_t\\)\n\\[\nh1 = g(W_hh0 + W_xx1) \\\\\nh2 = g(W_hh1 + W_xx2) \\\\\nh3 = g(W_hh2 + W_xx3) \\\\\n\\]\nLet \\(L\\) loss function\\[\n\\frac{\\partial L_{}}{\\partial W_{h}} =\n\\frac{\\partial L_{}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial W_{h}} +\n\\frac{\\partial L_{}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial W_{h}} +\n\\frac{\\partial L_{}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}} \\frac{\\partial h_{1}}{\\partial W_{h}}\\\\\n\\phantom{1}\\\\\n\\frac{\\partial L}{\\partial W_{h}} = \\frac{1}{n} \\sum_{t=1}^{n} \\frac{\\partial L_{t}}{\\partial h_{t}} \\prod_{j=+1}^t \\frac{\\partial h_{j}}{\\partial h_{j-1}} \\frac{\\partial h_{}}{\\partial W_{h}}\n\\]gradients consistently 1, gradient multiplied \\(t\\) times explodes resulting overflow. gradients consistently less 1, gradient multiplied \\(t\\) times vanishes resulting underflow.","code":""}]
