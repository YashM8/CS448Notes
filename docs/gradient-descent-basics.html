<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Gradient Descent Basics | CS448Notes</title>
  <meta name="description" content="Chapter 2 Gradient Descent Basics | CS448Notes" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Gradient Descent Basics | CS448Notes" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Gradient Descent Basics | CS448Notes" />
  
  
  

<meta name="author" content="Yash Mali" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="iterations-of-gradient-descent..html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CS448 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> CPSC 448 Notes</a></li>
<li class="chapter" data-level="2" data-path="gradient-descent-basics.html"><a href="gradient-descent-basics.html"><i class="fa fa-check"></i><b>2</b> Gradient Descent Basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="gradient-descent-basics.html"><a href="gradient-descent-basics.html#gradient-descent-background"><i class="fa fa-check"></i><b>2.1</b> Gradient descent background</a></li>
<li class="chapter" data-level="2.2" data-path="gradient-descent-basics.html"><a href="gradient-descent-basics.html#showing-gradient-descent-reduces-the-objective-function."><i class="fa fa-check"></i><b>2.2</b> Showing gradient descent reduces the objective function.</a></li>
<li class="chapter" data-level="2.3" data-path="gradient-descent-basics.html"><a href="gradient-descent-basics.html#what-learning-rate-to-use"><i class="fa fa-check"></i><b>2.3</b> What learning rate to use?</a></li>
<li class="chapter" data-level="2.4" data-path="gradient-descent-basics.html"><a href="gradient-descent-basics.html#convergence-rate"><i class="fa fa-check"></i><b>2.4</b> Convergence rate</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="iterations-of-gradient-descent..html"><a href="iterations-of-gradient-descent..html"><i class="fa fa-check"></i><b>3</b> Iterations of gradient descent.</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CS448Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient-descent-basics" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Gradient Descent Basics<a href="gradient-descent-basics.html#gradient-descent-basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="gradient-descent-background" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Gradient descent background<a href="gradient-descent-basics.html#gradient-descent-background" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gradient descent is an iterative optimization algorithm that was first proposed in 1847 by Cauchy. The algorithm can be summarized as follows:</p>
<p><span class="math display">\[
w^t = w^{t-1} - \alpha_t \nabla f(w^t) \\
\text{  For t = 1, 2, 3 ...}
\]</span></p>
<p>Where we are trying to find a set of parameters <span class="math inline">\(w\)</span> that minimize the function <span class="math inline">\(f\)</span>, also called the objective function. <span class="math inline">\(\nabla f\)</span> is the gradient of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(w\)</span> and the subscript <span class="math inline">\(t\)</span> is the iteration number.</p>
<p>The main idea is to move in the opposite direction of the steepest ascent of the function <span class="math inline">\(f\)</span>. How much you move during each iteration. is controlled by two things. The first is the steepness of gradient which we cannot control after we have chosen the objective function. The second is the parameter <span class="math inline">\(\alpha_t\)</span>. It is also called th learning rate.</p>
<p>The time complexity of <strong>each</strong> iteration is only <span class="math inline">\(O(d)\)</span> after computing the gradient. Where <span class="math inline">\(d\)</span> is the number of parameters. We can stop if we have made very little progress <span class="math inline">\(||f(w^t) - f(w^{t-1})||\)</span> is very small.</p>
<p>Intuitively gradient descent can be thought of as standing on the top of a canyon while being blindfolded and using your leg to find the steepest slope downwards locally. Then taking a small step in that direction and repeating the process again. In this analogy the canyon can be thought of as modelled my as 3D objective function.</p>
</div>
<div id="showing-gradient-descent-reduces-the-objective-function." class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Showing gradient descent reduces the objective function.<a href="gradient-descent-basics.html#showing-gradient-descent-reduces-the-objective-function." class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us assume that the objective function is Lipschitz continuous. Intuitively it means that a this function does not change in gradient arbitrarily fast. Formally it means that there has to be a a real values number <span class="math inline">\(L\)</span> that satisfies:</p>
<p><span class="math display">\[
\nabla f(w) − \nabla f(v) \le L||w − v||
\]</span>
For twice continuously differentiable <span class="math inline">\((C^2)\)</span> functions, using the mean value theorem, we can show:
<span class="math display">\[
||\nabla^2 f (w)|| \le L \Longrightarrow \nabla^2 f (w) \le LI
\]</span>
So we can bound quadratic functions of the form below using:</p>
<p><span class="math display">\[
d^T \nabla^2f(w) d ≤ d^T (LI)d = Ld^Td = L||d^2||
\]</span>
Using multivariate Taylor expansion:</p>
<p><span class="math display">\[
f(v) = f(w) + \nabla f(w)^T (v - w) + \frac{1}{2} (v - w)^T \nabla^2 f(u) (v - w),\text{where } u \in [v, w]
\\
f(v) \le f(w) + \nabla f(w)^T (v - w) + \frac{L}{2} ||v-w||^2
\]</span>
This is also known as the descent lemma.</p>
<p>This inequality give us an upper bound on <span class="math inline">\(f\)</span> that can be minimized by <span class="math inline">\(\alpha_t = \frac{1}{L}\)</span>. Using the above equations we can show that gradient descent reduces the objective with one iteration:</p>
<p><span class="math display">\[
w^t = w^{t-1} - \alpha_t \nabla f(w^k) \\
w^t = w^{t-1} - \frac{1}{L} \nabla f(w^t)\\
f(w^t) \le f(w^{t-1}) + \nabla f(w^{t-1})^T (w^{t} - w^{t-1}) + \frac{L}{2} ||w^{t}-w^{t-1}||^2\\
\text{Now we can use, } w^{t}-w^{t-1} = - \frac{1}{L} \nabla f(w^{t-1})\\
f(w^t) \le f(w^{t-1}) - \nabla f(w^{t-1})^T \frac{1}{L} \nabla f(w^{t-1}) + \frac{L}{2} ||\frac{1}{L} \nabla f(w^{t-1})||^2\\
f(w^t) \le f(w^{t-1}) - \frac{1}{L} ||\nabla f(w^{t-1})||^2 + \frac{1}{2L} ||\nabla f(w^{t-1})||^2\\
f(w^t) \le f(w^{t-1}) - \frac{1}{2L} ||\nabla f(w^{t-1})||^2\\
\]</span>
This shows that with every iteration we are guaranteed to make progress with the learning rate <span class="math inline">\(\alpha_t = \frac{1}{L}\)</span> when the gradient is non-zero.</p>
</div>
<div id="what-learning-rate-to-use" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> What learning rate to use?<a href="gradient-descent-basics.html#what-learning-rate-to-use" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Using <span class="math inline">\(\alpha_t = \frac{1}{L}\)</span> is impractical since computing <span class="math inline">\(L\)</span> is very expensive. The step size we get from this approach is usually very small. A more practical solution is to approximate <span class="math inline">\(L\)</span>. Starting with an initial guess <span class="math inline">\(\hat{L}\)</span>. Then before you take a step, check if the progress bound is satisfied.</p>
<p><span class="math display">\[
f(w^t -\frac{1}{\hat{L}} \nabla f(w^{t})) \le f(w^{t}) - \frac{1}{2L} ||\nabla f(w^{t})||^2\\
\text{Where } w^t -\frac{1}{\hat{L}} f(w^{t}) \text{ is a potential } w^{t+1}
\]</span>
Then, double <span class="math inline">\(\hat{L}\)</span> if the condition is not satisfied.</p>
<p><strong>Armijo Backtracking</strong></p>
<ol style="list-style-type: decimal">
<li><p>Start <strong>each iteration</strong> with Start each iteration with a large <span class="math inline">\(\alpha\)</span> so as to be optimistic of that fact that we are not in the worst case where we need a small step size.</p></li>
<li><p>Half <span class="math inline">\(\alpha\)</span> until the Armijo condition is satisfies. This is given by:</p></li>
</ol>
<p><span class="math display">\[
f(w^t - \alpha \nabla f(w^{t})) \le f(w^t) - \alpha \gamma ||\nabla f(w^t)||^2 \\
\text{For } \gamma \in (0, \frac{1}{2}]
\]</span></p>
<p>This allows us to vary the learning rate in such a way that the new set of parameters <span class="math inline">\(w^{t+1}\)</span> have sufficiently decreased the objective function going the current parameters <span class="math inline">\(w^t\)</span>.</p>
</div>
<div id="convergence-rate" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Convergence rate<a href="gradient-descent-basics.html#convergence-rate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Using out pogress bound, <span class="math inline">\(f(w^t) \le f(w^{t-1}) - \frac{1}{2L} ||\nabla f(w^{t-1})||^2 \Longrightarrow ||\nabla f(w^{t-1})||^2 \le 2L[f(w^{t-1}) - f(w^t)]\)</span></p>
<p>Let’s consider the smallest squared gradient norm <span class="math inline">\(\min_{\mathbf{j \in {[0, t-1]}}} \|\nabla f(\mathbf{w^j})\|^2\)</span>. This is the change in the objective function is the smallest.</p>
<p>Trivally, this will be smaller than the average squared gradient norm:</p>
<p><span class="math display">\[
\min_{\mathbf{j \in {[0, t-1]}}} \|\nabla f({w^j})\|^2 \le \frac{1}{t} \sum_{k=1}^t ||\nabla f({w^{k-1}})||^2 \le \frac{2L}{t} \sum_{k=1}^t [\nabla f({w^{k-1}}) - f(w^k)] \\
\frac{2L}{t} \sum_{k=1}^t [\nabla f({w^{k-1}}) - f(w^k)] = \frac{2L}{t} [f(w^0) - f(w^t)] \\
\text{Also, } f(w^t) \ge f^* \text{ where } f^* \text{ is objective value for optimal } w^* \\
\min_{\mathbf{j \in {[0, t-1]}}} \|\nabla f({w^j})\|^2 \le \frac{2L}{t} [f(w^0) - f^*] = O(1/t)
\]</span>
It is not the last iteration that satisfies the inequality. It can be satisfied at any point of the optimization process. The last iteration however does have the lowest <span class="math inline">\(f\)</span> value. This also does not imply that we will find the global optima. We could be minimizing a objective that has local optima.</p>
<p>We usually stop the iterative process when the norm is below some small value <span class="math inline">\(\epsilon\)</span>.</p>
<p><span class="math display">\[
\min_{\mathbf{j \in {[0, t-1]}}} \|\nabla f({w^j})\|^2 \le \frac{2L}{t} [f(w^0) - f^*] \le \epsilon \\
t \geq \frac{2L[f(w_0) - f^*)]}{\epsilon}\\
t = O(1/\epsilon)
\]</span>
To satisfy out stopping condition. <span class="math inline">\(t = O(1/\epsilon)\)</span> is called the <strong>iteration complexity</strong> of the algorithm. For least squares, the cost of computing a gradient is <span class="math inline">\(O(nd)\)</span> where <span class="math inline">\(n\)</span> is the nuber of data points and <span class="math inline">\(d\)</span> is the dimensionality of the data. The total cost is <span class="math inline">\(O(nd \times 1/\epsilon)\)</span></p>
<p>Another way to measure the rate of convergence is by the limit of the ratio of successive errors:
<span class="math display">\[
\lim_{k \to \infty} \frac{f(w_{k+1}) - f(w^*)}{f(w_k) - f(w^*)} = \rho.
\]</span></p>
<p>Different values of <span class="math inline">\(\rho\)</span> give us different rates of convergence:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\rho=1\)</span>, it is called a sublinear rate. Which means we need <span class="math inline">\(O(1/\epsilon)\)</span> iterations.</li>
<li>If <span class="math inline">\(\rho \in (0, 1)\)</span> it is called a linear rate. Which means we need <span class="math inline">\(O(log(1/\epsilon))\)</span> iterations.</li>
<li>If <span class="math inline">\(\rho = 0\)</span>, it is called a superlinear rate. Which means we need <span class="math inline">\(O(log(log(1/\epsilon))\)</span> iterations.</li>
</ol>
<p>Having <span class="math inline">\(f(w_t) - f(w^*) = O(1/t)\)</span> gives a sublinear convergence rate. The longer you run the algorithm, the less progress it makes.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="iterations-of-gradient-descent..html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/YashM8/CS448Notes/blob/main/02-basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/YashM8/CS448Notes/blob/main/02-basics.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
