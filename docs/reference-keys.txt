cpsc-448-notes
gradient-descent-analysis
gradient-descent-background
showing-gradient-descent-reduces-the-objective-function.
what-learning-rate-to-use
convergence-rate
improving-gradient-descent
oracle-model-of-computation
heavy-ball-method
conjugate-gradient-heavy-ball-with-optimal-parameters
nesterov-accelerated-gradient
coordinate-optimization
definition-and-examples
analyzing-coordinate-descent
randomized-cd-progress
gauss-southwell-greedy-coordinate-descent
stochastic-gradient-methods
introduction
progress-bound-for-sgd
convergence-of-sgd-for-pl-functions
when-to-stop
mini-batches-and-batch-growing
variation-in-mini-batch-approximation
overparameterization
overparameterization-and-sgd
faster-sgd-for-overparameterized-models
stochastic-line-search
variations-on-sgd
stochastic-average-gradient
variance-reduced-stochastic-gradient
approximating-the-hessian
cheap-hessian-approximation-1---diagonal-hessian
cheap-hessian-approximation-2---preconditioning
cheap-hessian-approximation-3---mini-batch-hessian
hessian-free-newton-methods-truncated-newton
quasi-newton-methods
barzilai-borwein-method
bfgs-quasi-newton-method
projected-gradient-based-algorithms
projected-gradient
l1-regularization-to-a-constrained-problem
active-set-identification-and-backtracking
accelerating-projection-methods
projected-sgd-and-cd
frank-wolfe-method
global-optimization-and-subgradients
considering-minimizing-lipschitz-continuous-functions
subgradient-methods
linear-convergence
using-multiple-subgradients
proximal-gradient-methods
convergence-rate-1
dualty
supremum-and-infimum
optimization-demo
gradient-descent-with-momentum
gradient-descent-with-nesterov-acceleration
nesterov-with-restart
newtons-method
damped-newtons-method
coordinate-descent
connvolutional-neural-networks
basics
convolutions
convolutional-layers
pooling-layers
example
conclusion
recurrent-neural-networks
background
introduction-1
types-of-rnns
one-to-many
many-to-one
many-to-many
problems-with-basic-rnns
lstm---long-short-term-memory
gated-recurrent-units
attention-with-rnns
introduction-2
downsides-of-this-attention.
self-attention-and-transformers
self-attention-mechanism
multi-head-attention
tips-for-numerical-stability
transformers
conclution-and-drawbacks
project
